<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1, shrink-to-fit=no'>
<meta name='description' content='Sylvain Calinon'>
<meta name='author' content='Sylvain Calinon'>
<meta name='keywords' content='Sylvain Calinon, robotics, machine learning, robot learning, frugal learning, human-robot collaboration, learning from demonstration, programming by demonstration, Idiap, model-based optimization, Riemannian geometry, tensor methods, tensor factorization, optimal control, ergodic control'>
<meta name='theme-color' content='#343a40'>
<link rel='icon' href='images/favicon.ico'>
<title>Sylvain Calinon</title>
<link href='css/bootstrap.min.css' rel='stylesheet'>
<link href='css/main-template.css' rel='stylesheet'>
<link href='font-awesome/css/font-awesome.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css?family=Lobster|Raleway' rel='stylesheet'> 

<link rel='stylesheet' href='https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css' integrity='sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X' crossorigin='anonymous'>
<script defer src='https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js' integrity='sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz' crossorigin='anonymous'></script>
<script defer src='https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js' integrity='sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05' crossorigin='anonymous'></script>
<script>
let macros = {
	'\\tp': '\\text{\\tiny{#1}}',
	'\\trsp' : '\\top',
	'\\psin' : '\\dagger',
	'\\eqref': '\\href{###1}{(\\text{#1})}',
	'\\ref': '\\href{###1}{\\text{#1}}',
	'\\label': '\\htmlId{#1}{}'
};
document.addEventListener('DOMContentLoaded', function() {
	renderMathInElement(document.body, {
		// customised options
		trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
		macros: macros,
		// • auto-render specific keys, e.g.:
		delimiters: [
			{left: '$$', right: '$$', display: true},
			{left: '$', right: '$', display: false},
			{left: '\\(', right: '\\)', display: false},
			{left: '\\begin{equation}', right: '\\end{equation}', display: true},
			{left: '\\begin{equation*}', right: '\\end{equation*}', display: true},
			{left: '\\begin{align}', right: '\\end{align}', display: true},
			{left: '\\begin{align*}', right: '\\end{align*}', display: true},
			{left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
			{left: '\\begin{gather}', right: '\\end{gather}', display: true},
			{left: '\\begin{CD}', right: '\\end{CD}', display: true},
			{left: '\\[', right: '\\]', display: true}
		],
		// • rendering keys, e.g.:
		throwOnError : false
	});
});
</script>

</head>

<body>
<nav class='navbar navbar-expand-lg navbar-dark bg-dark fixed-top'>
<button class='navbar-toggler navbar-toggler-right' type='button' data-toggle='collapse' data-target='#navbarsExampleDefault' aria-controls='navbarsExampleDefault' aria-expanded='false' aria-label='Toggle navigation'>
<span class='navbar-toggler-icon'></span>
</button>
<a class='navbar-brand' href='index.htm'>
<h3 style='display: inline-block;'>Sylvain Calinon</h3>
<img src='images/pbd-thumbnail02.png' style='display: inline-block; margin-left: 10px;'>
</a>

<div class='collapse navbar-collapse' id='navbarsExampleDefault'>
<ul class='navbar-nav mr-auto'>
<li class='nav-item'><a class='nav-link' href='index.htm'>News</a></li>
<li class='nav-item'><a class='nav-link' href='cv.htm'>CV</a></li>
<li class='nav-item'><a class='nav-link' href='research.htm'>Research</a></li>
<li class='nav-item'><a class='nav-link' href='publications.htm'>Publications</a></li>
<li class='nav-item'><a class='nav-link' href='teaching.htm'>Teaching</a></li>
<li class='nav-item'><a class='nav-link' href='book.htm'>Book</a></li>
<li class='nav-item'><a class='nav-link' href='videos.htm'>Videos</a></li>
<li class='nav-item'><a class='nav-link' href='codes.htm'>Codes</a></li>
<li class='nav-item'><a class='nav-link' href='open-positions.htm'>Open positions </a></li>
<li class='nav-item active'><a class='nav-link' href='contact.htm'>Contact/Links <span class='sr-only'>(current)</span></a></li>	
</ul>
</div>
</nav>

<div class='container'>
<div class='main-template'>



<p class='float-right'><b>English</b> / <a href='drozbot_fr.htm'>Français</a></p>

<h1>drozBot, the portraitist robot</h1>

<div class='row'>
<div class='col-md-3'>
<img class='figure-img img-fluid' src='images/drozbot-setup01.jpg'>
</div>
<div class='col-md-9'>
<p class='text-justify'>drozBot is a robot that draws with fast and smooth gestures similar to those of an artist. From a photo, it creates a portrait of the person by tracing a series of lines in China ink on a sheet of paper.</p> 
<p class='text-justify'>Guided by artificial intelligence and a movement model inspired by human handwriting, it can transform an image into an abstract drawing. The result is surprising: expressive, dynamic, and natural strokes, which give the impression that the robot knows how to draw rather than execute a series of commands.</p>
<p class='text-justify'>drozBot uses technologies developed in the <i>Robot Learning & Interaction group</i> at Idiap. The goal is to quickly generate portraits with three main objectives:
<ul> 
<li>To recognize the person who was drawn</li> 
<li>To recognize the distinctive style defined by the artist</li>
<li>To recognize the quality of the dynamic strokes based on the final portrait</li>
</ul> 
</p>
<p class='text-justify'>The latter goal is achieved by combining approaches from several scientific disciplines, blending robotics, optimal control, and motion science. As with calligraphy, even if a person only looks at the final portrait, a successful sketch must reflect the fluidity and skill of the movement that enabled the portrait to be created.</p> 
</div>
</div>


<div class='row'>
<div class='col-md-12'>

<p class='text-justify'>This aspect of motion generation is at the heart of the research work of Idiap's <i>Robot Learning & Interaction</i> group. The group focuses on human-robot collaboration and movement representations to facilitate the transfer of manipulation tasks, for various applications ranging from industry to human assistance.</p> 


<h3>International Collaboration: Between Art, Science & Technology</h3>

<p class='text-justify'>In collaboration with various international researchers from the University of London, Goldsmiths College (<a href="https://www.gold.ac.uk/computing/people/berio-daniel-/" target="_blank">Daniel Berio</a> and <a href="https://www.gold.ac.uk/computing/people/f-folleymarie/" target="_blank">Frederic Fol Leymarie</a>) or the University of Konstanz (<a href="https://www.cgmi.uni-konstanz.de/" target="_blank">Michael Stroh and Oliver Deussen</a>), within the framework of projects such as <a href="https://www.doc.gold.ac.uk/eacva/wp/index.php/collaborations/" target="_blank">EACVA (Embodied Agents in Contemporary Visual Art)</a>, the Robot Learning & Interaction group at Idiap explores how to extend human-robot collaboration to fields related to art and creativity. The robot is seen here as a new tool for artists to explore new methods of creation.</p>

<p class='text-justify'>In the same way that an artist decides which brushes, pens, or pencils to use, as well as additional guides such as using a ruler to draw straighter lines or a compass to draw regular arcs, the robot can be used as a new type of guide and assistance.</p> 

<p class='text-justify'>To achieve this, our research explores which interfaces can make it easy to teach a robot the assistance the artist wants. For example, in the form of a style that is shown to the robot (learning by demonstration), which the robot then has to repeat for new content or on new parts of the canvas.<p> 

<p class='text-justify'>In this research vision, our group is interested in the development of intuitive technologies so that a robot can learn by imitation, combined with interfaces to be able to easily reprogram these machines, without requiring knowledge of robotics or computer language ("no-code" approach).</p>


<h3>Interdisciplinary collaboration at Idiap</h3>

<p class='text-justify'>drozBot revolves around various research themes at Idiap on artificial intelligence, for example drawing inspiration from research results on human-robot social interaction (<a href="https://emmanuel-senft.github.io/" target="_blank">Emmanuel Senft</a>), on biometrics (<a href="https://www.idiap.ch/en/scientific-research/biometrics-security-and-privacy/index_html" target="_blank">Sébastien Marcel</a>), on perception (<a href="https://www.idiap.ch/~odobez/" target="_blank">Jean-Marc Odobez</a>), or on semantic and symbolic representations for neural network models (<a href="https://www.idiap.ch/~afreitas/" target="_blank">André Freitas</a>, <a href="https://www.idiap.ch/en/about/people/1124" target="_blank">Paola Merlo</a>).</p>  


<h3>Smooth Movements and Gestures</h3>

<p class='text-justify'>In artistic fields, deep learning approaches, with generative models, have democratized (for better or worse) the generation of images from textual descriptions. These AI models generate images in the form of pixels. A robot, on the other hand, generates paths and trajectories, with control at the level of its joints. Moreover, unlike digital art, the robot must face the real world, with geometric and physical constraints, for example, having to compensate for the effects of gravity when moving its arm holding an ink-soaked brush.</p> 

<p class='text-justify'>The scientific challenge is to define mathematical representations that mimic the capabilities of human movement, with fluid and rapid gestures for an aesthetic rendering of China ink drawings. For this, the movement to be generated is a crucial aspect, controlling both the orientation and trajectory of the brush, as well as the pressure applied on the paper.</p> 


<h3>AI and Robotics</h3>

<p class='text-justify'>To help the robot plan the series of strokes it should generate, other AI tools are used to automatically detect, from the image coming from a camera, what the pose of the person is in the image, and where the most important elements of the face are located, such as the eyes, mouth, and nose. Other elements of the image, such as the background, hair, or glasses, are detected in a similar way. This information helps the robot determine what concentration and precision of strokes are required in the portrait, focusing on the most important elements and producing a quicker sketch for less important elements of the drawing.<p>

<p class='text-justify'>The robot's planned strokes must then be transformed into smooth and fast movements to mimic the artist's gestures, skillfully handling the brush and deftly using an inkwell to maintain the ideal amount of ink.</p>


<h3>Scientific publications:</h3>

<p><a href='https://calinon.ch/paper3111.htm' target='_blank'>Berio, D., Clivaz, G., Stroh, M., Deussen, O., Plamondon, R., Calinon, S. and Leymarie, F.F. (2025). <strong>Image-driven robot drawing with rapid lognormal movements</strong>. In Proc. IEEE Intl Symp. on Robot and Human Interactive Communication (Ro-Man).</a></p>

<p><a href='https://calinon.ch/paper4074.htm' target='_blank'>Berio, D., Stroh, M., Calinon, S., Fol Leymarie, F., Deussen, O. and Shamir, A. (2025). <strong>Neural Image Abstraction Using Long Smoothing B-splines</strong>. ACM Transactions on Graphics (ToG).</a></p>

<p><a href='https://calinon.ch/paper3111.htm' target='_blank'>Berio, D., Clivaz, G., Stroh, M., Deussen, O., Plamondon, R., Calinon, S. and Leymarie, F.F. (2025). <strong>Image-driven robot drawing with rapid lognormal movements</strong>. In Proc. IEEE Intl Symp. on Robot and Human Interactive Communication (Ro-Man).</a></p> 

<p><a href='https://calinon.ch/paper3110.htm' target='_blank'>Berio, D., Calinon, S., Plamondon, R. and Leymarie, F. F. (2025). <strong>Differentiable rasterization of minimum-time sigma-lognormal trajectories</strong>. In Proc. 22nd Conference of the International Graphonomics Society (IGS).</a></p>

<p><a href='https://calinon.ch/paper3109.htm' target='_blank'>Calinon, S. (2025). <strong>Movement Generation and Drawing in Robotics</strong>. In Proc. 22nd Conference of the International Graphonomics Society (IGS).</a></p> 

<p><a href='https://calinon.ch/paper4055.htm' target='_blank'>Löw, T., Maceiras, J. and Calinon, S. (2022). <strong>drozBot: Using Ergodic Control to Draw Portraits</strong>. IEEE Robotics and Automation Letters (RA-L), 7:4, 11728-34.</a></p>


<h3>Contact:</h3> 

<p class='text-justify'><a href="https://calinon.ch/" target="_blank">Sylvain Calinon</a>, Senior Research Scientist at Idiap, heading the <i>Robot Learning & Interaction</i> group. Email: <a href="mailto:sylvain.calinon@idiap.ch" target="_blank">sylvain.calinon@idiap.ch</a></p>

<p class='text-justify'>drozBot is the result of a collaboration between <a href="https://www.idiap.ch/~gclivaz/" target="_blank">Guillaume Clivaz</a>, <a href="https://www.idiap.ch/~jmaceiras/" target="_blank">Jérémy Maceiras</a>, <a href="https://tobiloew.ch/" target="_blank">Tobias Löw</a>, <a href="https://www.enist.org/" target="_blank">Daniel Berio</a>, and <a href="https://calinon.ch/" target="_blank">Sylvain Calinon</a>.</p>

<p class='text-justify'>This project is supported by the <a href="https://www.snf.ch/en" target="_blank">Swiss National Science Foundation (SNSF) </a> as part of an Agora project with <a href="https://www.phaenomena.ch/en" target="_blank">Phänomena</a>.</p>  

</div>
</div>


<!--
<div class='row'>
<div class='col-md-6'><img class='figure-img img-fluid' src='images/drozbot_museum01.jpg'></div>
<div class='col-md-6'><img class='figure-img img-fluid' src='images/drozbot_museum02.jpg'></div>
</div>

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/K-WPry8ltXU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>

<p class='text-justify'>drozBot draws portraits with a conventional pen and paper. It first takes a snapshot of the person by framing the face through recognition of the principal facial features (eyes, nose, mouth). The portrait in a digital format is then converted to a <b>fluid movement mimicking the gestures of an artist</b>.</p>

<p class='text-justify'>This installation aspires <b>to explore the frontiers between optimality and randomness</b> in Artificial Intelligence (AI) applications. The learning mechanisms behind AI often use random processes to search for solutions (stochastic sampling during optimization). This randomization component behind these learning algorithms for example allow a robot to find solutions quickly without having to test each possible option. drozBot instead relies on a different perspective, by solving an <b>optimal control problem</b> aiming at determining the pen strokes to generate to best cover the canvas for each portrait to draw. Interestingly, this generative process does not rely on randomness while the result appears to be composed of aleatory strokes, with <b>doodles revealing the global objective only when the drawing is finished</b>. These doodles appear to be random when they are observed from close distance, but they are in reality optimal, with a unique result with respect to the global objective given to the robot.</p>

<p class='text-justify'>To reach this drawing capability, the objectives of the robot are multiple. First, it has to generate smooth gestures while controlling the orientation and pressure of the pen on the paper. Then, it has to define a series of strokes reproducing the different contrasts in the image, by forecasting a greater number of overlapping paths in darker regions of the image, while reducing the overall length of the movement.</p>

<p class='text-justify'>When observing the robot at work, the doodles appear meaningless at first sight. The global objective is only revealed with the progression of the drawing. Indeed, the <b>appearing chaos of the first strokes disappear with the ensemble view of the final portrait</b>.</p> 

<img class='figure-img img-fluid' src='images/drozBot-portraits02-web.jpg'><br><br>
<img class='figure-img img-fluid' src='images/drozbot_setup_en01.jpg'><br><br>



<div class='p-3 bg-light text-dark border border-secondary text-justify'>
In robotics, this technique is called <b>ergodic control</b>. It stands out from standard approaches in control that define the task in the form of a point (or a series of points) to reach precisely. In the ergodic control approach, instead of a point, <b>a region to explore is given as instruction to the robot</b>.
The objective of the robot is then to move in order <b>to cover this region in an optimal manner</b>. We use these same algorithms in our research work to handle insertion tasks in industrial applications, which allow the robot to automatically compensate for imprecise sensing information with a search behavior around the insertion points.
</div>
<br>

<div class='p-3 bg-light text-dark border border-secondary text-justify'>
drozBot was part of the exhibition <a href='https://www.museedelamain.ch/fr/1591/Intelligence-Artificielle' target='_blank'>Intelligence Artificielle: Nos Reflets Dans La Machine</a>, UNIL-CHUV Museum of the Hand in Lausanne, from April 2022 to September 2023.
</div>
-->

<!--
<img class='figure-img img-fluid' src='images/drozBot-part1.jpg'><br><br>
avec un bras articulé produisant une série de gribouillis révélant progressivement l'identité de la personne
<img class='figure-img img-fluid' src='images/drozBot-part2.jpg'><br><br>
<img class='figure-img img-fluid' src='images/drozBot-part3.jpg'><br><br>-->

</div><!-- /.main-template -->
</div><!-- /.container -->


<!-- Bootstrap core JavaScript placed at the end of the document so the pages load faster -->
<script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>
<script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
<script src="js/bootstrap.min.js"></script>
<!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
<!--<script src="assets/js/ie10-viewport-bug-workaround.js"></script>-->
</body>
</html>
