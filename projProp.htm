<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1, shrink-to-fit=no'>
<meta name='description' content='Sylvain Calinon'>
<meta name='author' content='Sylvain Calinon'>
<meta name='keywords' content='Sylvain Calinon, robotics, machine learning, robot learning, human-robot interaction, human-robot collaboration, learning from demonstration, programming by demonstration, artificial intelligence, Idiap, model-based optimization, Riemannian geometry, tensor methods, tensor factorization, optimal control, ergodic control'>
<meta name='theme-color' content='#343a40'>
<link rel='icon' href='images/favicon.ico'>
<title>Sylvain Calinon</title>
<link href='css/bootstrap.min.css' rel='stylesheet'>
<link href='css/main-template.css' rel='stylesheet'>
<link href='font-awesome/css/font-awesome.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css?family=Lobster|Raleway' rel='stylesheet'> 

<link rel='stylesheet' href='https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css' integrity='sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X' crossorigin='anonymous'>
<script defer src='https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js' integrity='sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz' crossorigin='anonymous'></script>
<script defer src='https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js' integrity='sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05' crossorigin='anonymous'></script>
<script>
let macros = {
	'\\tp': '\\text{\\tiny{#1}}',
	'\\trsp' : '\\top',
	'\\psin' : '\\dagger',
	'\\eqref': '\\href{###1}{(\\text{#1})}',
	'\\ref': '\\href{###1}{\\text{#1}}',
	'\\label': '\\htmlId{#1}{}'
};
document.addEventListener('DOMContentLoaded', function() {
	renderMathInElement(document.body, {
		// customised options
		trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
		macros: macros,
		// • auto-render specific keys, e.g.:
		delimiters: [
			{left: '$$', right: '$$', display: true},
			{left: '$', right: '$', display: false},
			{left: '\\(', right: '\\)', display: false},
			{left: '\\begin{equation}', right: '\\end{equation}', display: true},
			{left: '\\begin{equation*}', right: '\\end{equation*}', display: true},
			{left: '\\begin{align}', right: '\\end{align}', display: true},
			{left: '\\begin{align*}', right: '\\end{align*}', display: true},
			{left: '\\begin{alignat}', right: '\\end{alignat}', display: true},
			{left: '\\begin{gather}', right: '\\end{gather}', display: true},
			{left: '\\begin{CD}', right: '\\end{CD}', display: true},
			{left: '\\[', right: '\\]', display: true}
		],
		// • rendering keys, e.g.:
		throwOnError : false
	});
});
</script>

</head>

<body>
<nav class='navbar navbar-expand-lg navbar-dark bg-dark fixed-top'>
<button class='navbar-toggler navbar-toggler-right' type='button' data-toggle='collapse' data-target='#navbarsExampleDefault' aria-controls='navbarsExampleDefault' aria-expanded='false' aria-label='Toggle navigation'>
<span class='navbar-toggler-icon'></span>
</button>
<a class='navbar-brand' href='index.htm'>
<h3 style='display: inline-block;'>Sylvain Calinon</h3>
<img src='images/pbd-thumbnail02.png' style='display: inline-block; margin-left: 10px;'>
</a>

<div class='collapse navbar-collapse' id='navbarsExampleDefault'>
<ul class='navbar-nav mr-auto'>
<li class='nav-item'><a class='nav-link' href='index.htm'>News</a></li>
<li class='nav-item'><a class='nav-link' href='cv.htm'>CV</a></li>
<li class='nav-item active'><a class='nav-link' href='research.htm'>Research <span class='sr-only'>(current)</span></a></li>
<li class='nav-item'><a class='nav-link' href='publications.htm'>Publications</a></li>
<li class='nav-item'><a class='nav-link' href='teaching.htm'>Teaching</a></li>
<li class='nav-item'><a class='nav-link' href='book.htm'>Book</a></li>
<li class='nav-item'><a class='nav-link' href='videos.htm'>Videos</a></li>
<li class='nav-item'><a class='nav-link' href='codes.htm'>Codes</a></li>
<li class='nav-item'><a class='nav-link' href='open-positions.htm'>Open positions </a></li>
<li class='nav-item'><a class='nav-link' href='contact.htm'>Contact/Links</a></li>	
</ul>
</div>
</nav>

<div class='container'>
<div class='main-template'>



<!--<p>This page presents our research on robot learning and interaction. The descriptions of past research themes are available <a href='research_prev.htm'>here</a>.</p>-->
 
<h3>EPFL Students Projects Proposals</h3>

<p class='text-justify'>The projects below are available for either Bachelor/Master semester projects or Master thesis projects (the content will be adjusted accordingly). Suggestions of other projects (or variants of existing projects) are also welcome, as long as they fit within the group's research interests.</p>

<p><b>Contact:</b> sylvain.calinon<i class="fa fa-at fa-fw"></i>epfl.ch</p>
<br>


<hr><br><br>
<h4><b>Wavelets as basis functions for applications in robotics</b></h4>

<br>
<div class='row'>

<div class='col-md-12'>

<figure class='figure'>
<img class='figure-img img-fluid' src='images/Bezier_1D2D3D01.jpg'>
</figure>

<p class='text-justify'>Basis functions can be used to encode signals in a compact manner through a weighted superposition of basis functions, acting as a dictionary of simpler signals that are superposed to form more complex signals. The dictionary can be any set of basis functions, including radial basis functions (RBFs), Fourier basis functions, or Bernstein basis functions (used for Bézier curves), see first reference below for more details. Basis functions can for example be used to encode trajectories, whose input is a 1D time variable and whose output can be multidimensional. Basis functions can also be used to encode signals generated by multivariate inputs. For example, a Bézier surface uses two input variables to cover a spatial range and generates an output variable describing the height of the surface within this rectangular domain. This surface (represented as a colormap in the figure) can be constructed from a 1D input signal by leveraging the Kronecker product operation.</p>

<p class='text-justify'>The figure in the above shows examples using various input dimensions to encode signed distance functions (SDFs). On the central plot, several equidistant contours are displayed as closed paths, with the one corresponding to the object contour (distance zero) represented in blue. On the right plot, several isosurfaces are displayed as 3D shapes, with the one corresponding to the object surface (distance zero) represented in blue. The analytic expression provided by the proposed encoding can be used to express the derivatives as analytic expressions, which is useful for control and planning problem, such as moving closer or away from objects, or orienting the robot gripper to be locally aligned with the surface of an object.</p>

<p><b>Goals of the project:</b></p>
<p class='text-justify'>The project proposes to extend the above approach to the use of wavelet basis functions and to study the property of wavelets in the context of robot manipulation skills. Wavelets encompass both spatial and spectral properties, which makes it a good candidate to encode functions at different resolutions. The approach will be tested in the context of signed distance functions (see second reference).</p> 

<p class='text-justify'><b>Prerequisites:</b> Signal processing, programming in Python, C++ or Matlab/Octave</p>

<p><b>References:</b></p>
<p class='text-justify'><a href='https://calinon.ch/papers/Calinon_MMchapter2019.pdf' target='_blank'>Calinon, S. (2019). Mixture Models for the Analysis, Edition, and Synthesis of Continuous Time Series. Bouguila, N. and Fan, W. (eds). Mixture Models and Applications, pp. 39-57. Springer.</a></p>
<p class='text-justify'><a href='https://arxiv.org/abs/2307.00533' target='_blank'>Li, Y., Zhang, Y., Razmjoo, A. and Calinon, S. (2023). Learning Robot Geometry as Distance Fields: Applications to Whole-body Manipulation. ArXiv 2307.00533.</a></p>

<p><b>Level:</b> Bachelor/Master (semester project or PDM)

<p><b>Contact:</b> sylvain.calinon<i class="fa fa-at fa-fw"></i>epfl.ch</p>

</div>

</div>




<br><hr><br><br>
<h4><b>A robot manipulator writing texts using a pen</b></h4>

<br>
<div class='row'>

<div class='col-md-12'>

<figure class='figure'>
<img class='figure-img img-fluid' src='images/writingRobot01.jpg'>
</figure>

<p class='text-justify'>This project proposes to develop a robot that can take any text as input and write the corresponding sequence of letters on a piece of paper. In order to look natural, the use of a typeface vector font such as Hershey (<a href='' target='_blank'>https://pypi.org/project/Hershey-Fonts/</a>, <a href='' target='_blank'>https://en.wikipedia.org/wiki/Hershey_fonts</a>) will be investiagted as first starting point. Such a vector font format can be employed to represent alphabet characters by a set of strokes forming the skeleton of the letters instead of their outlines (as in the conventional font formats).</p> 

<p><b>Goals of the project:</b></p>
<p class='text-justify'>Several aspects will have to be considered in the project, such as placement and segmentation of the text on the page, the planning and control approach using inverse kinematics, as well as estimation of the writing result through the camera embedded within the robot arm. The project will be implemented with a 6-axis UFactory Lite-6 robot (<a href='' target='_blank'>https://www.ufactory.cc/lite-6-collaborative-robot</a>) available at Idiap.</p>

<p class='text-justify'><b>Prerequisites:</b> Linear algebra, programming in Python or C++</p>

<p><b>References:</b></p>
<p class='text-justify'><a href='https://rcfs.ch/doc/rcfs.pdf' target='_blank'>Calinon, S. (2023). Learning and Optimization in Robotics - Lecture notes.</a></p>
<p class='text-justify'><a href='https://rcfs.ch/' target='_blank'>Robotics codes from scratch (RCFS)</a></p>

<p><b>Level:</b> Bachelor/Master (semester project or PDM)

<p><b>Contact:</b> sylvain.calinon<i class="fa fa-at fa-fw"></i>epfl.ch</p>

</div>

</div>





<br><hr><br><br>
<h4><b>Tensor trains for human-guided optimization in robotics applications</b></h4>

<br>
<div class='row'>

<div class='col-md-12'>

<figure class='figure'>
<img class='figure-img img-fluid' src='images/TT-pipeline-small01.jpg'>
</figure>

<p class='text-justify'>The project proposes to extend the Tensor Train for Global Optimization (TTGO) approach (see reference below) to a human-guided learning strategy. The approach will be tested with control and planning problems using robot manipulators. The formulation transforms the minimization of a cost function into the maximization of a  probability density function. Hence, the problem of finding the minima of a cost function is transformed into the problem of sampling the maxima of the density function. The approach does not require gradients to be computed and can find multiple optima, which can be used for global optimization problems.</p> 

<p class='text-justify'>Learning and optimization problems in robotics are characterized by two types of variables: 1) task parameters representing the situation that the robot encounters, typically related to environment variables such as locations of objects, users or obstacles; and 2) decision variables related to actions that the robot takes, typically related to a controller acting within a given time window, or the use of basis functions to describe trajectories in control or state spaces. For each change of task parameters, decision variables need to be recomputed as fast as possible, so that the robot can fluently collaborate with users and can swiftly react to changes in its environment. In TTGO, the density function is modeled in the offline phase using a tensor train (TT) that learns the structure between the task parameters and the decision variables. It then allows conditional sampling over the task parameters with priority for higher-density regions. This property is used for fast online decision-making, with local Gauss-Newton optimization to refine the results, see the above figure for an overview. TTGO allows the distribution of computation into offline and online phases. It can cope with a mix of continuous and discrete variables for optimization thus providing an alternative to mixed-integer programming.</p>

<p><b>Goals of the project:</b></p>
<p class='text-justify'>An important advantage of TTGO is that the underlying TT-cross method used to approximate the distribution in TT format can easily be extended to various forms of human-guided learning, which will be investigated in this project. The goal will be to test whether the original autonomous learning strategy of TT-Cross can be extended to a human-guided learning strategy, by letting the user sporadically specify task parameters or decision variables within the iterative process. The first case can be used to provide a scaffolding mechanism for robot skill acquisition. The second case can be used for the robot to ask for help in specific situations.</p>

<p class='text-justify'><b>Prerequisites:</b> Linear algebra, optimization, programming in Python</p>

<p><b>Reference:</b></p>
<p class='text-justify'><a href='https://sites.google.com/view/ttgo' target='_blank'>Shetty, S., Lembono, T., Löw, T. and Calinon, S. (2023). Tensor Train for Global Optimization Problems in Robotics. arXiv:2206.05077.</a></p>

<p><b>Level:</b> Bachelor/Master (semester project or PDM)

<p><b>Contact:</b> sylvain.calinon<i class="fa fa-at fa-fw"></i>epfl.ch</p>

</div>

</div>




<br><hr><br><br>
<h4><b>Ergodic control for robot exploration</b></h4>

<br>
<div class='row'>

<div class='col-md-12'>

<figure class='figure'>
<img class='figure-img img-fluid' src='images/ergodic-small02.png'>
</figure>

<p class='text-justify'>A conventional tracking problem in robotics is characterized by a target to reach, requiring a controller to be computed to reach this target. In ergodic control, instead of providing a single target point, a probability distribution is given to the robot, which must cover the distribution in an efficient way. Ergodic control thus consists of moving within a spatial distribution by spending time in each part of the distribution in proportion to its density (namely, ``tracking a distribution'' instead of ``tracking a point''). The resulting controller generates natural exploration behaviors, which can be exploited for active sensing, localization, surveillance, insertion tasks, etc.</p>

<p class='text-justify'>In robotics, ergodic control can be exploited in a wide range of problems requiring the automatic exploration of regions of interest. This is particularly helpful when the available sensing information is not accurate enough to fulfill the task with a standard controller, but where this information can still guide the robot towards promising areas. In a collaborative task, it can also be used when the operator's input is not accurate enough to fully reproduce the task, which then requires the robot to explore around the requested input (e.g., a point of interest selected by the operator). For picking and insertion problems, ergodic control can be applied to move around the picking/insertion point, thereby facilitating the prehension/insertion. It can also be employed for active sensing and localization (either detected autonomously, or with help by the operator). Here, the robot can plan movements based on the current information density, and can recompute the commands when new measurements are available (i.e., updating the spatial distribution used as target).</p> 

<p><b>Goals of the project:</b></p>
<p class='text-justify'>Ergodic control has originally been formulated as a Spectral Multiscale Coverage (SMC) objective. Later, another ergodic control formulation has been proposed, formulated as a Heat Equation Driven Area Coverage (HEDAC) problem. This project proposes to study the pros and cons for these techniques to solve robot manipulation problems.</p>

<p class='text-justify'><b>Prerequisites:</b> Control theory, signal processing, programming in Python, C++ or Matlab/Octave</p>

<p><b>References:</b></p>
<p class='text-justify'><a href='https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=16462e855562e7f2031dccb8215bfe2ea97fea39' target='_blank'>G. Mathew and I. Mezic (2009). Spectral multiscale coverage: A uniform coverage algorithm for mobile sensor networks. In Proc. IEEE Conf. on Decision and Control.</a></p>
<p class='text-justify'><a href='https://ieeexplore.ieee.org/document/7786872' target='_blank'>S. Ivić, B. Crnković, and I. Mezić (2007). Ergodicity-based cooperative multiagent area coverage via a potential field. IEEE Trans. on Cybernetics.</a></p>

<p><b>Level:</b> Bachelor/Master (semester project or PDM)

<p><b>Contact:</b> sylvain.calinon<i class="fa fa-at fa-fw"></i>epfl.ch</p>

</div>

</div>



<!--
<hr><br>
<h4>GEOMETRIC ALGEBRA FOR DISTANCE SENSORS CALIBRATION IN ROBOT MANIPULATION APPLICATIONS</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<p>Sensors play an important role in robotics, since they are needed by robots in order for them to perceive the environment and interact with it. It is therefore essential that the sensor readings are as accurate as possible. Achieving a high accuracy makes it necessary to perform a good calibration of the sensors.</p> 
<p>Calibration usually means determining either the intrinsic or extrinsic parameters of the sensor. In this particular case we are interested in finding the extrinsic parameters of an array of time of flight sensors that is attached to a Franka Emika serial manipulator. While it is possible to roughly determine the position of the array by taking measurements, the position and orientation of the sensors on the array cannot be measured
accurately due to possible deformations during attaching or manufacturing the array. The challenge therefore lies in finding the exact positions and orientations of all sensors by refining a rough initial estimate.</p> 
<p>There are various ways to tackle this problem, however in its essence it always is a geometric problem. Hence, the idea to solve this problem is to use an algebra that is made for geometry, namely geometric algebra. It is formalism that is currently resurfacing and gaining popularity in robotics. Its strength lies among other things in the possiblity to represent geometric primitives such as lines, planes and circles and their intersections in a parameterization
and exception free way. This property makes it the ideal tool for the calibration of sensors that take distance measurements along a line or more generally a cone.</p> 
<p>Thus, the goal of this project is the derivation of an algorithm for the fast and reliable extrinsic calibration of an array of time of flight sensors on a kinematic chain, as well as investigating the usage of geometric algebra as a tool for solving this problem. The expected outcome will include an implementation of the algorithm that has been tested thoroughly on the hardware that is provided in our lab, i.e. the sensor array and the robot.</p>
<p><b>Keywords:</b> geometric algebra, model-based optimization, time of flight sensors, parameters identification, robot manipulation</p>
</div>

</div>


<hr><br>
<h4>LOAD INERTIAL PARAMETERS IDENTIFICATION FOR A FORCE-CONTROLLED ROBOT</h4>

<br>
<div class='row'>

<div class='col-md-8'>
<p>In contact-rich robotics tasks, it is crucial to have an accurate measurement of forces and torques exchanged between the robot and its environment to feed those measurements back in a control loop. However, force/torque sensors placed between the tip of the robot and the end-effector measure not only those interaction forces, but also static (e.g. due to gravity) and dynamic (e.g. inertial, Coriolis and centrifugal) effects of the end-effector due to its non-negligible mass. In order to compensate for those measurement errors, the dynamic parameters of the end-effector such as mass, inertia and center-of-mass offset have to be known to be able to predict the resulting forces/torques corrupting the sensor measurement. Typically, those parameters are unknown or inaccurate, such that a model identification procedure is required. For the identification procedure, data has to be collected by moving the robot such that trajectories of motion and reaction forces can be recorded.</p>
<p>In this project, a load inertial parameter estimation algorithm will be implemented that (1) autonomously excites the robot sufficiently to collect dynamic motion/force data, (2) applies derivation filters (offline) to compute high-quality robot joint acceleration signals, and (3) solves an optimization problem to obtain an estimate of the inertial parameters of the robots end-effector.</p>
<p>The algorithm will be implemented (Python or C++) and tested on a 7-axis Franka Emika robot equipped with 6-axis force/torque sensor from Bota systems. Potentially, the work can be extended by using IMU measurements online to have a better estimate of the current acceleration of the robot, which will improve the prediction quality when using the learned dynamics model.</p>
<p><b>Keywords:</b> parameters identification, force-torque sensing, robot manipulation</p>
</div>

<div class='col-md-4'>
<figure class='figure'>
<img class='figure-img img-fluid' src='images/load_parameter_identification.png'>
</figure>
<p><b>Reference:</b></p>
<p><a href='https://doi.org/10.1007/978-3-540-30301-5_15' target='_blank'>Hollerbach, J., Khalil, W. and Gautier, M. (2008) Model Identification. Springer Handbook of Robotics, pp. 321-344.</a></p>
</div>

</div>


<hr><br>
<h4>KOOPMAN MODEL LEARNING WITH EQUATION LEARNER NETWORKS</h4>

<br>
<div class='row'>

<div class='col-md-8'>
<p>In robotics, optimal control is a popular and robust approach to generate trajectories and the corresponding controller required to achieve a task. Optimal control consists of optimizing over the states and the controls such that they minimize a cost function which describe a task given a dynamics model of the robot. However, in some cases, this model is either not available, or imperfect because of the unmodeled forces in the joints or the tools used by the robot.</p> 
<p>Recently, Koopman operators have become increasingly popular in the identification of the dynamics model. The framework consists of augmenting the original set of variables composing the state space so that the nonlinear system can be expressed as a linear system in this augmented state space. The lifting functions used for this have been investigated in several works using non-parametric basis functions such as monomials, polynomials, radial basis functions or Fourier series basis which require no or little tuning of their parameters in general. However, such basis functions are difficult to design, and one needs to come up with a huge dictionary of variables to alleviate this problem.  Other works proposed to use parametric basis function such as neural networks for the lifting functions extending the capability of such basis functions significantly. However, they are hard to learn, easy to overfit and not interpretable.</p>
<p>A promising approach which is shown to contain advantages of both non-parametric and parametric liftings is equation learner networks (EQL). The project proposes to investigate the use of EQL networks in Koopman operator framework to learn a robust and interpretable dynamics model from small amount of data. The approach will be implemented on a 7-axis Franka Emika robot to learn the residual dynamics model of the robot which is caused by frictions and other unmodeled imperfections.</p>
<p><b>Keywords:</b> optimal control, neural networks, Koopman operators</p>
</div>

<div class='col-md-4'>
<figure class='figure'>
<img class='figure-img img-fluid' src='images/EquationLearnerNetwork01.png'>
</figure>
<p><b>Reference:</b></p>
<p><a href='http://proceedings.mlr.press/v80/sahoo18a/sahoo18a.pdf' target='_blank'>Sahoo, S.S., Lampert, C.H., Martius, G. (2018). Learning equations for extrapolation and control. Intl Conf. on Machine Learning (ICML).</a></p>
</div>

</div>


<hr><br>
<h4>TENSOR-VARIATE DICTIONARY LEARNING OF MOVEMENT PRIMITIVES</h4>

<br>
<div class='row'>

<div class='col-md-7'>
<p>In robotics, it is useful to represent movements as a superposition of basis functions. Often, the set of basis functions is predefined. The corresponding learning problem consists of estimating how to combine the set of basis functions to form a desired movement. The set of basis functions (dictionary of movement primitives) can be learned together with their combination. To do this, a sparse coding approach for dictionary learning will be exploited, as presented in the video and paper below. The approach will be evaluated on robot motion data.</p>
<p>The standard dictionary learning problem is formulated with data in the form of input x and output y, which are then used to estimate a new output y_new, given a new input x_new. The project proposes to extend this approach to the more general case in which x and y are multidimensional arrays (also called tensors, see slides below). The approach will be evaluated on robot motion data.</p>
<p><b>Keywords:</b> tensor methods, movement primitives, sparse coding</p>
</div>

<div class='col-md-5'>
<figure class='figure p-3'>
<img class='figure-img img-fluid' src='images/tensor-factorization01.jpg'>
</figure>
<p><b>References:</b></p>
<p><a href='misc/EE613/EE613-tensorRegression.pdf' target='_blank'>Slides from a lecture on tensor-variate regression</a></p>
<p><a href='https://arxiv.org/pdf/1711.10781.pdf' target='_blank'>Rabanser, S., Shchur, O. and Günnemann, S. (2017). Introduction to Tensor Decompositions and their Applications in Machine Learning. arXiv:1711.10781.</a></p>
</div>

</div>
-->




<!--
<hr><br>
<h4>CONTROL AND PLANNING WITH A GAUSSIAN PROCESS IMPLICIT SURFACE REPRESENTATION OF THE ENVIRONMENT</h4>

<br>
<div class='row'>

<div class='col-md-8'>
<p>In this project, we will consider a robot manipulation problem with sensors mounted at the end-effector of the robot. These sensors can typically take the form of cameras, proximity sensors, or tactile sensors. With these sensors, the robot can build a model of its surrounding environment by active perception, with a trade-off between exploration and exploitation that depend on the required accuracy and locality to achieve the task while reacting appropriately to encountered perturbations.</p> 
<p>Several approaches can be used to represent the environment and obstacles surrounding a robot, including geometric shapes, occupancy voxel grids, or implicit surface representations. The project proposes to study the use of implicit surface representations, which are typically implemented as Gaussian processes (GPs), where kernel functions such as radial basis functions (RBFs) are used to measure distances. GPs provide gradients to let the robot know how to avoid an obstacle, move along a surface, or establish contact with the closest surface. GPs also provide a measure of uncertainty that can be exploited within a minimal intervention control strategy (i.e., exploiting redundancy and coordination by rejecting only the perturbations that would have an effect on the achievement of the task).</p> 
<p>This semester project proposes to investigate two improvements of GPIS based on RBFs: 1) the use of a sparse GP approach to reduce the computation time when large numbers of datapoints have been collected; 2) the extension to other kernels that would better take into account the geometry of the problem.</p>
<p>This project takes place within the MEMMO project (memory of motion).</p>
</div>

<div class='col-md-4'>
<figure class='figure'>
<img class='figure-img img-fluid' src='images/GPIS01.jpg'>
<figcaption class='figure-caption'>Implicit contour representation with Gaussian process regression.</figcaption>
</figure>
<p><b>References:</b></p>
<p><a href='http://gpss.cc/gpip/abstract/owilliams.pdf' target='_blank'>Original GPIS approach</a></p>
<p><a href='https://arxiv.org/abs/2010.11487' target='_blank'>Use of GPIS in robotics</a></p>
<p><a href='https://arxiv.org/abs/2008.09848' target='_blank'>Sparse GPs</a></p>
</div>

</div>
-->


<!--
<hr><br>
<h4>MOTION OPTIMIZATION WITH A LEGIBILITY PERSPECTIVE</h4>

<br>
<div class='row'>

<div class='col-md-2'>
<figure class='figure'>
<img class='figure-img img-fluid' src='images/LQT_legibility01.png'>
</figure>
</div>

<div class='col-md-10'>
<p>Standard motion optimization problems in robotics focus on the use of cost functions that measure how well a task is executed (e.g., positions to reach, viapoints to pass through, orientations to maintain). In a manipulation task, the typical goal will be to generate movements that efficiently execute the task. We propose to extend the definition of these costs to include human-robot interaction aspects.</p>
<p>The project will investigate the problem of generating movements for the robot that would allow an external observer to understand quickly the intention of the robot. This requires the investigation of legibility costs to be used to generate motions capable of reducing ambiguity (e.g., by exaggerating the movement to make it more legible to a user interacting with the robot). The side image shows how a legible movement can be generated to reduce ambiguity in the intended movement (here, to emphasize that the blue object will be grapsed instead of the orange object).</p>
<p>This project takes place within the ROSALIS project (robot skills acquisition through active learning and social interaction strategies).</p>
</div>

</div>
-->





<!--
<hr><br>
<h4>SMARTPHONE INTERFACE USING AUGMENTED REALITY</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<figure class='figure'>
<img class='figure-img img-fluid' src='images/projProp-smartphone-interface01.png'>
</figure>
<p>We have developed an augmented reality interface running on Android smartphones to display a virtual robot (left image). The app relies on ARCore, Google's toolkit for building augmented reality applications on Android and iOS devices. This toolkit is used to estimate the location of the phone and to render 3D graphics on top of the camera's image displayed on the screen.</p>   
<p>The aim of the project is to extend this interface to let the user move the virtual robot to a desired configuration (see right images as an illustration). Several options can be considered, such as clicking on the robot articulations and dragging them to their desired positions, or drawing a stick figure on top of the image, which is then interpreted to set the desired pose of the robot.</p>
<p>The Android smartphone will be programmed in Java (knowledge of either Java or C++ is required for the project).</p>
<p>The developed interface will be tested to change the pose of a real 7-axis Panda robot (Franka Emika), by first setting and visualizing the motion of the virtual robot on the smartphone, and then running the motion on the real robot. A basic interface between the mobile phone and the robot is already available for the project (by using the ROS middleware). The proposed approach will finally be evaluated with inexperienced user to determine if it is accurate and easy-to-use.</p>
<p><b>Keywords:</b> augmented reality, smartphone interfaces, robotics, inverse kinematics, machine learning</p>
<p><b>Link and reference:</b></p>
<p><a href='https://developers.google.com/ar' target='_blank'>ARCore (Google)</a></p>
<p><a href='paper7006.htm' target='_blank'>Thoo, Y.J., Maceiras, J., Abbet, P., Racca, M., Girgin, H. and Calinon, S. (2021). Online and Offline Robot Programming via Augmented Reality Workspaces. arXiv:2107.01884.</a></p>
</div>

</div>
-->


<!--
<hr><br>
<h4>SMARTPHONE INTERFACE FOR A CARICATURIST ROBOT</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<figure class='figure'>
<img class='figure-img img-fluid' src='images/drozBot-web01.jpg'>
</figure>
<p>This semester project takes places within a larger project that aims at creating <a href='drozbot.htm' target='_blank'>a portraitist robot for a museum exhibition</a>. In this project, a CNN is used to extract a set of facial landmarks from a camera mounted at the tip of a 7-axis robot. After centering the face on the photo taken by the robot, the image is converted to a series of 2D paths (pen strokes) through an ergodic control formulation that mimics natural drawing patterns. These 2D paths are then reproduced on a canvas in front of the robot, using a pen held by its gripper, through an iLQR planning technique (iterative linear quadratic regulator).</p> 
<p>The goal of this semester project is to replace the camera held by the robot with a smartphone interface that would allow the users to select or edit a caricature interactively. Mediapipe Face Mesh (https://google.github.io/mediapipe/solutions/face_mesh.html) will be used as a CNN-based approach to extract a set of facial landmarks from the phone's camera. From the facial landmarks, a new image will be generated by removing automatically the background and by deforming the original image based on a subset of the facial landmarks, where the distortions can be either predefined or edited by the user. The developed interface will be evaluated to determine if it is intuitive for first-time users.</p>
<p><b>Keywords:</b> smartphone interface for robotics applications, image processing, deep learning, nonphotorealistic rendering</p>
<p><b>Link:</b></p>
<p><a href='https://google.github.io/mediapipe/' target='_blank'>MediaPipe (Google)</a></p>
</div>

</div>
-->



<!--
<hr><br>
<h4>ORIENTATION AND FORCE CONTROL OF A ROBOT HOLDING A PAINT BRUSH</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<figure class='figure'>
<img class='figure-img img-fluid' src='images/drozBot-web01.jpg'>
</figure>
<p>This semester project takes places within a larger project that aims at creating <a href='drozbot.htm' target='_blank'>a portraitist robot for a museum exhibition</a>. In this project, a CNN is used to extract a set of facial landmarks from a camera mounted at the tip of a 7-axis robot. After centering the face on the photo taken by the robot, the image is converted to a series of 2D paths (pen strokes) through an ergodic control formulation that mimics natural drawing patterns. These 2D paths are then reproduced on a canvas in front of the robot, using a pen held by its gripper, through an iLQR planning technique (iterative linear quadratic regulator).</p> 
<p>The goal of this semester project is to study the extension of the approach to brush strokes, which requires to take into account the orientation of the brush when generating the paths to draw the portrait, as well as when controlling the robot holding the brush. To do so, the ergodic control formulation used to generate the paths from an image will be employed, as it allows patterns of different shapes to be considered for the drawing of an image. Additionally, the project will investigate if force information should be taken into account when moving the brush on the paper, by exploiting the torque sensing capability of the robot. The proposed approach will be evaluated both in simulation and on the real robot.</p>
<p><b>Keywords:</b> painting robot, nonphotorealistic rendering, ergodic control</p>
</div>

</div>
-->


<!--

<hr><br>
<h4>TENSOR FACTORIZATION FOR MULTI-TASK LEARNING</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<figure class='figure'>
<img class='figure-img img-fluid' src='images/tensor-factorization01.jpg'>
<figcaption class='figure-caption'>Left: Standard matrix factorization (singular value decomposition). Right: Tensor factorization.</figcaption>
</figure>
<p>Manipulation skills in robotics are encoded as a weighted superposition of movement primitives, where the problem consists of learning a dictionary of movement primitives together with the superposition weights. The current limitation is that each skill is learned individually, which limits the skills transfer capability.</p>
<p>The project proposes to address this limitation by relying on tensor methods, which will be used by the robot to devise a common dictionary to learn multiple skills in an incremental manner. Tensor methods are extensions of standard linear algebra techniques to arrays of higher dimension (typically, extension of singular value decomposition to arrays of more than two dimensions, capturing correlations between different dimensions in a compact way).</p> 
<p>This project takes place within the LEARN-REAL project (learning physical manipulation skills with simulators using realistic variations).</p>

<p><b>References:</b></p>
<p><a href='misc/EE613/EE613-tensorRegression.pdf' target='_blank'>Slides for course on tensor-variate regression (the slides contain also other topics)</a></p>
<p><a href='https://arxiv.org/pdf/1711.10781.pdf' target='_blank'>Rabanser, S., Shchur, O. and Günnemann, S., Introduction to Tensor Decompositions and their Applications in Machine Learning</a></p>
<p><a href='https://www.ijcai.org/Proceedings/2017/0484.pdf' target='_blank'>Zhao, C., Hospedales, T.M., Stulp, F. and Sigaud, O., Tensor Based Knowledge Transfer across Skill Categories for Robot Control</a></p>

</div>
</div>

<p><a href='https://www.youtube.com/watch?v=7a0_iEruGoM&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&index=61&t=0s' target='_blank'>Sparse coding video lectures from Hugo Larochelle (part 8.1-8.9)</a></p>
<p><a href='https://www.di.ens.fr/willow/pdfs/icml09.pdf' target='_blank'>Julien Mairal, Francis Bach, Jean Ponce and Guillermo Sapiro, "Online Dictionary Learning for Sparse Coding", ICML'2019</a></p>



----
Decoupling background and foreground information for a drawing robot

This semester project takes places within a larger project that aims at creating a portraitist robot for a museum exhibition: https://calinon.ch/drozbot.htm. In this project, a CNN is used to extract a set of facial landmarks from a camera mounted at the tip of a 7-axis robot. After centering the face on the photo taken by the robot, the image is converted to a series of 2D paths (pen strokes) through an ergodic control formulation that mimics natural drawing patterns. These 2D paths are then reproduced on a canvas in front of the robot, using a pen held by its gripper, through an iLQR planning technique (iterative linear quadratic regulator).  

The goal of this semester project is to exploit the facial landmarks extracted from the CNN to draw a foreground portrait on top of a background generated by texts used as repetitive patterns to fill the background part of the image. The proposed approach will be evaluated both in simulation and on the real robot.



----
Obstacle-aware inverse kinematics with implicit surface representation

Gaussian Process Implicit Surfaces (GPIS) is a technique allowing shapes to be modeled in a probabilistic manner. It relies on a set of datapoints annotated with values 0, 1 and -1 to define if the datapoint is on the border, inside or outside the shape. We can then use this model to estimate the distance of a point to the border of the shape, as well as estimating the direction to move toward or away from this border. This direction can be weighted with respect to the distance to the border, in order to obtain a vector field that is strong when close to the border (or inside the shape), and weak elsewhere. The distance information takes the form of a Gaussian distribution, meaning that we also get uncertainty information about this estimate. Thus, such estimates will be more trustworthy in areas covered by labeled points than in areas that were not demonstrated. The probabilistic formulation can also be exploited to define prior information about the shape, which is useful when only a few points is available to model the shape, or when datapoints representing a portion of the shape are missing. 

The GPIS approach is typically used in computer graphics to represent 3D shapes or 2D contour, but the approach can be used in any dimension, including the configuration space (joint angle space) of a robot. In this project, it is proposed to rely on this representation to let a robot move from any two points while avoiding fixed obstacles in the robot workspace, where GPIS is used to model the contours of the obstacles. To do this, the robot is first moved in the free regions of its workspace to collect datapoints with labels -1. It is then moved on the boundary of the admissible regions (labels 0). The points of the forbidden regions are then either demonstrated (when possible), or computed as centroids of the contour points (or groups of contour points). Two variants are then proposed for the controller:

GPIS in configuration space (variant 1):
In this variant, GPIS is modeled in a 7D space (corresponding to the 7 axes of the robot manipulator). A standard inverse kinematics (IK) solver is used at each time step to compute a velocity command in joint angle space. This command is then combined with the repulsive command provided by GPIS, either computed as a product of Gaussians (fusion of the two commands), or as a hierarchical control problem (by exploiting the nullspace matrix as an operator to prioritize obstacle avoidance).

GPIS in operational space (variant 2):
In this variant, GPIS is modeled in a 3D space (Cartesian coordinate system describing the task space of the robot). Instead of using a standard IK solver that takes into account only the end-effector of the robot, we extend the forward kinematics function to take into account several points on the kinematic chain of the robot (for example, each articulation). The gradient of this function is a Jacobian matrix, whose pseudoinverse defines a mapping between the desired velocities of these points in task space and the velocity commands in joint space. The desired velocities are determined by GPIS (collision avoidance for all points) and by the task to achieve (endeffector tracking a target). This inverse kinematics formulation can be extended to a weighted least-squares approach in which the weights are set with respect to the estimated distance of the points to the contour, so that the points on the kinematic chain that are far from obstacles will not have an impact on the resulting joint angle commands.

Both variants will first be tested with a 2-axis planar robot using the Matlab robotics toolbox (https://github.com/petercorke/robotics-toolbox-matlab or http://www.petercorke.com/RTB/). For inverse kinematics, the project will rely on demo_IK01.m, a simple IK example (incl. nullspace control), available in https://gitlab.idiap.ch/rli/pbdlib-matlab/. For GPIS, the project will rely on demo_GPR_closedShape02.m, also available in https://gitlab.idiap.ch/rli/pbdlib-matlab/.

References:
GPIS paper: https://www.microsoft.com/en-us/research/publication/gaussian-process-implicit-surfaces-2/ 
GPR: slides 3-27 from http://calinon.ch/misc/EE613/EE613-nonlinearRegressionII.pdf 
IK: slides 22-27 from http://calinon.ch/misc/EE613/EE613-linearRegressionI.pdf 

----
Koopman model learning with Equation Learner Networks
In robotics, optimal control is a popular and robust approach to generate trajectories and the corresponding controller required to achieve a task. Optimal control consists of optimizing over the states and the controls such that they minimize a cost function which describe a task given a dynamics model of the robot. However, in some cases, this model is either not available, or imperfect because of the unmodeled forces in the joints or the tools used by the robot. 

Recently, Koopman operators have become increasingly popular in the identification of the dynamics model. The framework consists of augmenting the original set of variables composing the state space so that the nonlinear system can be expressed as a linear system in this augmented state space. The lifting functions used for this have been investigated in several works using non-parametric basis functions such as monomials, polynomials, radial basis functions or Fourier series basis which require no or little tuning of their parameters in general. However, such basis functions are difficult to design, and one needs to come up with a huge dictionary of variables to alleviate this problem.  Other works proposed to use parametric basis function such as neural networks for the lifting functions extending the capability of such basis functions significantly. However, they are hard to learn, easy to overfit and not interpretable. 

A promising approach which is shown to contain advantages of both non-parametric and parametric liftings is equation learner networks (EQL). The project proposes to investigate the use of EQL networks in Koopman operator framework to learn a robust and interpretable dynamics model from small amount of data. The approach will be implemented on Panda robot to learn the residual dynamics model of the robot which is caused by frictions and other unmodeled imperfections. 

Sahoo, S. S., Lampert, C. H., Martius, G. Learning equations for extrapolation and control In Proc. \35th International Conference on Machine Learning, ICML 2018, Stockholm

http://proceedings.mlr.press/v80/sahoo18a/sahoo18a.pdf



--------------------------
Learning of non-photorealistic renderings for a caricaturist robot

This project aims to review, categorize and test existing approaches in deep learning to map an image to a set of trajectories that a robot can then draw on a canvas, by taking the example of portrait caricatures as an example of generative non-photorealistic rendering (NPR) problem (see CariGAN or WarpGAN as starting examples). Several data processing pipelines can be considered, with parts that can be specified manually, and others that can be learned from examples. It also includes the potential consideration of intermediary steps, such as detecting the location of facial features (e.g., by using Google's MediaPipe Face Mesh), image-to-image processing applying pencil/brush sketch rendering effects, or applying a distortion mask for a warping transformation defined explicitly. The project also aims at studying metrics or methods to compare these different approaches. The selected approach(es) will take into account the amount of data that these methods require, the availability of these data and of pretrained models, and the possibility of employing the selected method(s) on smartphones or CPUs (instead of GPUs).

----
Test and simulation of a torque-controlled quadruped robot

The Idiap Research Institute recently acquired a SOLO-12 robot, which is part of the open dynamic robot initiative (https://open-dynamic-robot-initiative.github.io/). It consists of an open torque-controlled modular robot architecture for legged locomotion research. The robot has been developed in the context of the MEMMO project in which Idiap participates (https://www.memmo-project.eu/). The assembly of the robot will be achieved by the PAL Robotics company (https://pal-robotics.com/), so that the platform will be ready to be used when the project starts.

The internship will consist of two parts. In a first phase, the robot will be simulated with PyBullet (https://pybullet.org/wordpress/), by following the instructions available on https://github.com/open-dynamic-robot-initiative and https://arxiv.org/pdf/1910.00093.pdf. In a second phase, the robot will be tested by installing the required software components to control the real robot. A set of movements will first be tested in simulation, and then executed on the robot.
 
Grimminger et al. (2020). An Open Torque-Controlled Modular Robot Architecture for Legged Locomotion Research. IEEE Robotics and Automation Letters, 5:2, 3650-3657.
https://arxiv.org/pdf/1910.00093.pdf

Pybullet
https://github.com/open-dynamic-robot-initiative/robot_properties_solo
https://github.com/open-dynamic-robot-initiative/robot_properties_solo/blob/master/demos/demo_simulate_solo8.py

LQT/iLQR with least-squares controller formulation, with a control primitives representation separating feedforward and feedback terms, for a quadruped robot application

----
<hr><br>
<h4>Interactive GUI for intuitive programming of robot manipulation tasks</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<figure class='figure p-3'>
<img class='figure-img img-fluid' src='images/'>
</figure>
<p>Industrial robots are often (re-)programmed through dedicated scripting languages and bulky teaching pendants. A recent trend, initiated by several industrial robotics companies, has been to simplify this programming aspect by proposing graphical user interfaces that are more intuitive to use. They often take the form of blocks that are connected in sequence, whose parameters can be modified by the user.</p>

<p>This project proposes to explore another way of representing a computer program composed of task primitives that are not only organized in series but also in parallel. The developed GUI will take inspiration from the timelines used in animation and video editing softwares. The GUI will be developed so that it can be used within a web browser (including tablets and smartphones), by relying on javascript and modern responsive web development toolkits such as bootstrap (https://getbootstrap.com/) and d3 (https://d3js.org/).</p>

<p>The developed interface will be used to control both a virtual robot and a 7-axis Panda robot from the Franka Emika company (https://www.franka.de/).</p>

</p>
<p><b>Keywords:</b> user interfaces, robot programming, robot manipulation, task primitives</p>
</div>

</div>




----
<hr><br>
<h4>CIGAR BOX JUGGLING WITH ROBOTS</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<figure class='figure p-3'>
<img class='figure-img img-fluid' src='images/projProp-cigarBoxes01.png'>
</figure>
<p>"Cigar box juggling" consists of manipulating three boxes, by holding two boxes while one box is free. It involves the juggler to skillfully move two of the boxes to change their configurations (see above sequence). For beginners, the movement is typically executed by moving the boxes up-and-down when changing the configuration, so that there is more time to change the orientation of the box being held (since the other hand will ``accompany'' the falling box). If the up-and-down motion has high amplitude, the movement to change the configuration is relatively slow.</p>
<p>This project proposes to explore whether such motion could be planned and executed on two 7-axis Panda robots (Franka Emika). The dynamical model will first be built to determine the motion to execute. Model predictive control will then be employed to control the robot with anticipation capability.</p>
<p><b>Keywords:</b> robot control, robot planning, skills learning and adaptation, model predictive control</p>
</div>

</div>


----
<hr><br>
<h4>TENSOR-VARIATE REGRESSION IN ROBOTICS APPLICATIONS</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<figure class='figure'>
<img class='figure-img img-fluid' src='images/tensor-factorization01.jpg'>
<figcaption class='figure-caption'>Left: Standard matrix factorization (singular value decomposition). Right: Tensor factorization.</figcaption>
</figure>
<p>Many regression problems in robotics can be formulated as that of collecting data in the form of input x and output y, which are then used to estimate a new output y', given a new input x'. This project proposes to extend regression to the more general case in which x and y are multidimensional arrays (also called tensors).</p> 
<p>Tensors are generalization of matrices to arrays of higher dimensions, where vectors and matrices correspond to 1st and 2nd-order tensors. When data are organized in matrices or arrays of higher dimensions (tensors), classical regression methods first transform these data into vectors, therefore ignoring the underlying structure of the data and increasing the dimensionality of the problem. This flattening operation typically leads to overfitting when only few training data are available.</p> 
<p>Tensor methods are well established in several machine learning fields such as image/video processing and recommendation systems, but this area of research is still at its infancy in robotics.</p> 
<p>The project proposes to test and evaluate different generalizations of the pseudoinverse for tensor-variate data (see references below), which will be exploited in the context of linear regression for robotics applications.</p>
<p><b>Keywords:</b> tensor methods, regression</p>
<p><b>References:</b></p>
<p><a href='https://arxiv.org/pdf/1701.01037.pdf' target='_blank'>Lock, E.F. (2018). Tensor-on-Tensor Regression. Computational and Graphical Statistics 27:3, pp. 638-647</a>.</p>
<p><a href='https://ieeexplore.ieee.org/document/8421595' target='_blank'>Yin, M., Gao, J., Xie, S. and Guo, Y. (2019). Multiview Subspace Clustering via Tensorial t-Product Representation. IEEE Trans. on Neural Networks and Learning Systems 30:3, pp. 851-864</a>.</p>
</div>

</div>


----
<hr><br>
<h4>LEARNING TO SEARCH: AN ERGODIC CONTROL APPROACH</h4>

<br>
<div class='row'>

<div class='col-md-7'>
<p>In ergodic control, the aim is to find a sequence of control commands u(t) so that the retrieved trajectory x(t) covers a bounded space X in proportion of a given spatial distribution phi(x). The objective is formulated as a tracking problem in the frequency domain, in which the goal is to match the frequency patterns by putting more importance on low frequency components than high frequency components.</p>
<p>The resulting controller produces a natural movement to explore the regions of interest. Interestingly, this search pattern does not rely on random moves, but is instead well defined mathematically as a tracking problem in the frequency domain (i.e., matching Fourier series coefficients).</p>
<p>The aim of this project is to combine learning from demonstration with ergodic control, by extracting from the demonstration the regions that we need to explore, and then letting ergodic control explores these regions. The approach will be tested in an insertion task with a 7-axis Panda robot (Franka Emika). The implementation will be in C++ or Matlab (example codes for ergodic control in C++ and Matlab will be provided).</p>
<p><b>Keywords:</b> robot control, learning from demonstration, Fourier series</p>
<p><b>Reference:</b></p>
<p><a href='http://calinon.ch/papers/Calinon_MMchapter2019.pdf' target='_blank'>Calinon, S., "Mixture Models for the Analysis, Edition, and Synthesis of Continuous Time Series", Bouguila, N. and Fan, W. (eds), Mixture Models and Applications, Springer, 2019</a></p>
</div>

<div class='col-md-5'>
<figure class='figure p-3'>
<img class='figure-img img-fluid' src='images/projProp-ergodicControl01.png'>
</figure>
</div>

</div>


----
<hr><br>
<h4>GROWBOTHUB ROBOTICS PROJECTS</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<figure class='figure'>
<img class='figure-img img-fluid' src='images/growbothub01.jpg'>
</figure>
<p>The <a href='https://growbothub.space/' target='_blank'>GrowbotHub project</a> was founded by a team of EPFL/UNIL students from the Légumes Perchés Association and took part in the first edition of Igluna, an international student project to build a habitat demonstrator for sustaining life in an extreme environment such as on the Moon. Igluna is coordinated by the Swiss Space Center at EPFL and GrowbotHub is one of its most successful projects.</p>
<p>Three robotics projects using a <a href='https://www.kuka.com/en-ch/products/robotics-systems/industrial-robots/lbr-iiwa' target='_blank'>7-axis KUKA LBR iiwa</a> are described below. The full list of GrowbotHub projects is available <a href='http://growbothub.space/mesmerize/semester-projects/' target='_blank'>here</a>.</p>

<strong>Project 1.7 - Control of the Kuka robot in coordination with the rest of the system</strong>

<p>The goal of this project is to create a controller for the Kuka robot by replying on the sunrise environment provided by Kuka. Since the controller will need to communicate with the rest of the system, it is proposed to use <a href='https://www.ros.org/' target='_blank'>ROS</a> as a middleware to allow communication between the different parts of the system. In a first stage, the controller will rely on inverse kinematics to control the position and orientation of the tool. In a second stage, the torque control capability of the robot will be exploited to combine force tracking and position tracking.</p>

<strong>Project 1.8 - Adaptive pick-and-place of vegetable pots with the Kuka robot</strong>

<p>The goal of this project is to determine and evaluate solutions to pick-up vegetable pots (incl. how to modify the pots for better prehension), and drop them at a desired location. It requires to take into account motion planning, together with the limitations of the robot, including gripper and joint limits. Several gripper solutions and configurations will be investigated and tested.</p>

<strong>Project 1.9 - Perception and control for a 3-finger gripper for harvesting vegetables</strong>

<p>This project aims at harvesting vegetables (e.g., carrots) using a 3-finger gripper, by detecting the gripping point and applying appropriate forces to collect the vegetables without damaging them. There are two variants in this project, focusing on either the perception aspect or the control aspect. The first variant of the project will focus on determining a gripping point (position and orientation of the gripper) from computer vision processing. The second variant of the project will determine which gripper, gripper configuration and fingertips are required for the task, and will control the robot to reach the desired gripping point.</p>
</div>
</div>


----
<hr><br>
<h4>HIDDEN MARKOV MODEL FOR MUSIC SCORE SYNCHRONIZATION IN A HUMAN-ROBOT COLLABORATION</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<figure class='figure p-3'>
<img class='figure-img img-fluid' src='images/projProp-robotPianist01.png'>
</figure>
<p>We are developing a human-robot collaboration experiment in which a person plays a melody on a piano while a robot plays the accompanying chords (see the robot setup in the above figure). For the synchronization to be effective, the robot has to anticipate its movements in accordance to the estimated tempo. </p>
<p>This project proposes to rely on the score (midi format) to initialize a hidden Markov model (HMM), which is then refined by the recordings of a person playing the same score. The goal is to encode the natural variations to improve the encoding and synthesis of the music track. This HMM will then be used in the experiment to estimate the time when the accompanying notes in the partition should be played by the robot. The approach will be implemented in Python or C++, by exploiting existing example codes in Matlab or C++ as starting point.</p>
<p><b>Keywords:</b> time series analysis, hidden Markov model (HMM), music synthesis, human-robot collaboration</p>
</div>

</div>


----
<hr><br>
<h4>CONVOLUTIONAL NEURAL NETWORKS AND TENSOR METHODS</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<figure class='figure p-3'>
<img class='figure-img img-fluid' src='images/projProp-TME01.png'>
</figure>
<p>Sensory data in robotics are typically organized as multidimensional arrays (arrays of sensors, multiple channels, time evolution of data, multiple coordinate systems, etc.). This led our group to investigate tensor methods (a field of research also referred to as multilinear algebra).</p>
<p>Tensors are generalization of matrices to arrays of higher dimensions, where vectors and matrices correspond to 1st and 2nd-order tensors. When data are organized in matrices or arrays of higher dimensions (tensors), classical regression methods first transform these data into vectors, therefore ignoring the underlying structure of the data and increasing the dimensionality of the problem. This flattening operation typically leads to overfitting when only few training data are available.</p>
<p>In the reference below, we investigated the use of a mixture of experts relying on tensorial representations. The aim is to take into account the underlying structure of the data, with an approach that remains efficient even when only few training data are available.</p>
<p>The left figure illustrates how ridge regression and logistic regression can be extended to tensor-variate data. The right figure shows that a tensor-variate mixture of experts can be built by using tensor regression as experts, and tensor logistic regression as gating functions.</p>
<p>The goal of this project is to compare this strategy against the use of a convolutional neural network (CNN), from both theoretical and practical perspectives. For this project, previous experience on deep learning (and in particular CNN) would be useful. The implementation can be either done with TensorFlow, PyTorch or Matlab.</p>
<p><b>Keywords:</b> convolutional neural network, tensor methods</p>
<p><b>Reference:</b></p>
<p><a href='http://calinon.ch/papers/Jaquier-arXiv2019.pdf' target='_blank'>Jaquier, N., Haschke, R. and Calinon, S., "Tensor-variate Mixture of Experts", arXiv:1902.11104, 2019</a></p>
</div>

</div>


----
<hr><br>
<h4>ONLINE IMITATION WITH OBJECT-AWARE MAPPING</h4>

<br>
<div class='row'>

<div class='col-md-7'>
<p>Online imitation consists of copying a motion in real-time (see above image). This problem is very challenging in robotics because the robot and the user have different kinematics (and dynamics) characteristics, requiring the mapping strategy to be carefully chosen. Imitating joint angle trajectories is usually not appropriate, because the joints of the robot are not necessary organized in the same way as the morphology of a person. For this reason, the most often considered mapping is to copy the position of the robot/user hands, described in the Cartesian space.</p>
<p>In this project, it is proposed to explore a new strategy that consists of creating a mapping considering the objects in the robot/user workspace. This idea arises from the observation that most of the time, the robot hands will be used to manipulate objects, and the locations of these objects will typically differ in the two workspaces (the one of the user, and the one of the robot). In order to cope with this mapping problem, it is proposed to represent the movement from the perspective of these objects (e.g., in a relative distance space to the objects). Several strategies will be investigated and tested with a 7-axis Panda robot (Franka Emika). The implementation will be done either in Python, C++ or Matlab.</p>
<p><b>Keywords:</b> imitation, learning from demonstration, motion retargeting, correspondence problems</p>
<p><b>Reference:</b></p>
<p><a href='http://calinon.ch/papers/Calinon-EncyclopediaRobotics2019.pdf' target='_blank'>Calinon, S., ``Learning from Demonstration (Programming by Demonstration)'', Ang, M.H., Khatib, O. and Siciliano, B. (eds), Encyclopedia of Robotics, Springer, 2019</a></p>
</div>

<div class='col-md-5'>
<figure class='figure p-3'>
<img class='figure-img img-fluid' src='images/projProp-BaxterImitation01.png'>
</figure>
</div>

</div>


----
Optimal control with keypoints learned from demonstration

Kinesthetic teaching consists of grabbing the robot arm to demonstrate a task while the robot collects proprioceptive information through its joint encoders. Demonstrating a full movement with kinesthetic teaching is cumbersome and often loses the fluidity and dynamics in the movement. This project proposes to use kinesthetic teaching only for defining a set of viapoints that the robot should pass through, and to rely on optimization to let the robot compute control commands to pass through the desired viapoints. To do this, a spacetime formulation of iLQR will be tested, which consists of iteratively solving linear quadratic regulation problems, resulting in a planning strategy that takes into account both time and space aspects to fulfill the task. 

The project will use a 7-axis Panda robot form the Franka Emika company. The approach will be tested in a set of manipulation tasks inspired by industrial applications.


----
Adaptive robot control to pour liquids from a bottle 

In manipulation, pouring skills are challenging because the generated movement and feedback depend on the quantity of liquid present in the container. We plan to study this skill in the context of a robot application in which a manipulator, holding a bottle filled with liquid, can pour the liquid into a glass. Such challenge requires the combination of several sources of perception, including proprioceptive information (estimating the remaining quantity of liquid in the bottle from its weight), and vision (as a way to correct the pouring behavior, as well as to determine when to stop pouring). 

The project proposes to study this challenge by using a 7-axis Panda robot from the Franka Emika company (https://franka.de/). This robot is endowed with torque sensors, which will be exploited to estimate the quantity of liquid in the bottle form its weight. This estimate will be used in a planning step to define an open-loop movement (anticipated movement). This movement will then be corrected during its execution, with the help of a simple vision system that will be developed in the project to determine when colored liquid pours out of the bottle (the location of the top of the bottle is known by proprioception, using the joint angle encoders of the robot). The controller will be developed in C++, by using the libfranka library (https://frankaemika.github.io/docs/overview.html).


----
<hr><br>
<h4>ROBOT ASSISTANCE FOR STANDING UP AND SITTING DOWN</h4>

<br>
<div class='row'>

<div class='col-md-6'>
<figure class='text-center'>
<img class='figure-img img-fluid' src='images/SWITCHproj_3D01.jpg'>
</figure>
</div>
<div class='col-md-6'>
<figure class='text-center'>
<img class='figure-img img-fluid' src='images/SWITCHproj_2D01.png'>
</figure>
</div>
<div class='col-md-12'>
<p>The project will explore the problem of assisting a user to stand up and sit down. This problem will be modeled as two kinematic chains, where only planar movements will be considered. One kinematic chain will represent a humanoid robot and the other will represent the user to assist. It is assumed that both have fixed feet on the ground and are connected to a common endeffector point representing their hands, meaning that the resulting system is a closed kinematic chain in which only a few of the articulations can be controlled.</p> 
<p>The assistance skill consists of moving the user from a static sitting pose to a static standing pose through the contact point. It involves challenging aspects of anticipation, shared control, initiation of movements, leader-follower behaviors and haptic communication. This project will concentrate on studying the dynamical motion aspects to achieve such assistance, which requires the consideration of inertia and center of mass movements with respect to the feet.</p>
</div>

</div>


----
<hr><br>
<h4>SUBSPACE LEARNING FOR ROBOT CONTROL APPLICATIONS</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<p>In robotics, movements can be represented as a dynamical system describing the evolution in time of the robot. Most often, the system is nonlinear, and the standard approach is to linearize the system so that locally, a linear system can be considered. Another approach, originally proposed by Koopman, is to augment the original set of variables composing the state space so that the nonlinear system can be expressed as a linear system in this augmented state space.</p>
<p>To do this, several approaches have been proposed, such as forming this augmented state space with polynomial or Fourier expansions of the original signal, or learning this augmented state space with autoencoders. Often, these approaches consider systems that estimate the next state based on the current state.</p>
<p>A promising approach recently proposed in the reference below is to consider a history of the previous states. This approach suggests to go beyond standard polynomial basis functions or Fourier basis functions by exploiting delay coordinates as basis functions, in the form of a factorization of a Hankel matrix. The resulting algorithm is surprisingly short and simple to implement. This project proposes to explore this approach in a robot control task with the 7-axis Panda robot (Franka Emika).</p>
<p><b>Keywords:</b> dynamical systems, robot control, subspace learning, delay embedding, time series analysis</p>
<p><b>References:</b></p>
<p><a href='https://arxiv.org/pdf/1608.05306.pdf' target='_blank'>Steven L. Brunton, Bingni W. Brunton, Joshua L. Proctor, Eurika Kaiser, and J. Nathan Kutz, "Chaos as an Intermittently Forced Linear System", Nature Communications, 2017</a></p>
<p><a href='https://www.youtube.com/watch?v=831Ell3QNck' target='_blank'>Hankel Alternative View of Koopman (HAVOK) Analysis (video)</a></p>
</div>

</div>

-->


</div><!-- /.main-template -->
</div><!-- /.container -->


<!-- Bootstrap core JavaScript placed at the end of the document so the pages load faster -->
<script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>
<script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
<script src="js/bootstrap.min.js"></script>
<!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
<!--<script src="assets/js/ie10-viewport-bug-workaround.js"></script>-->
</body>
</html>
