<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1, shrink-to-fit=no'>
<meta name='description' content='Sylvain Calinon'>
<meta name='author' content='Sylvain Calinon'>
<meta name='keywords' content='Sylvain Calinon, robotics, machine learning, robot learning, human-robot interaction, human-robot collaboration, learning from demonstration, programming by demonstration, artificial intelligence'>
<meta name='theme-color' content='#343a40'>
<link rel='icon' href='images/favicon.ico'>
<title>Sylvain Calinon</title>
<link href='css/bootstrap.min.css' rel='stylesheet'>
<link href='css/main-template.css' rel='stylesheet'>
<link href='font-awesome/css/font-awesome.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css?family=Lobster|Raleway' rel='stylesheet'> 
<script type='text/x-mathjax-config'>
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script type='text/javascript' async src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
</head>
<body>
<nav class='navbar navbar-expand-lg navbar-dark bg-dark fixed-top'>
<button class='navbar-toggler navbar-toggler-right' type='button' data-toggle='collapse' data-target='#navbarsExampleDefault' aria-controls='navbarsExampleDefault' aria-expanded='false' aria-label='Toggle navigation'>
<span class='navbar-toggler-icon'></span>
</button>
<a class='navbar-brand' href='index.htm'>
<h3 style='display: inline-block;'>Sylvain Calinon</h3>
<img src='images/pbd-thumbnail02.png' style='display: inline-block; margin-left: 10px;'>
</a>

<div class='collapse navbar-collapse' id='navbarsExampleDefault'>
<ul class='navbar-nav mr-auto'>
<li class='nav-item'><a class='nav-link' href='index.htm'>News</a></li>
<li class='nav-item'><a class='nav-link' href='cv.htm'>CV</a></li>
<li class='nav-item active'><a class='nav-link' href='research.htm'>Research <span class='sr-only'>(current)</span></a></li>
<li class='nav-item'><a class='nav-link' href='publications.htm'>Publications</a></li>
<li class='nav-item'><a class='nav-link' href='teaching.htm'>Teaching</a></li>
<li class='nav-item'><a class='nav-link' href='book.htm'>Book</a></li>
<li class='nav-item'><a class='nav-link' href='videos.htm'>Videos</a></li>
<li class='nav-item'><a class='nav-link' href='codes.htm'>Codes</a></li>
<li class='nav-item'><a class='nav-link' href='open-positions.htm'>Open positions </a></li>
<li class='nav-item'><a class='nav-link' href='contact.htm'>Contact/Links</a></li>	
</ul>
</div>
</nav>

<div class='container'>
<div class='main-template'>

<!--<p>This page presents our research on robot learning and interaction. The descriptions of past research themes are available <a href='research_prev.htm'>here</a>.</p>-->

<h3>EPFL Student Project Proposals</h3>

The descriptions below are available for either semester projects or master thesis projects (the content will be adjusted accordingly). Suggestions of other projects (or variants of existing projects) are also welcome, as long as they fit within the group's research interests.





<hr><br>
<h4>ROBOT ASSISTANCE FOR STANDING UP AND SITTING DOWN</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<figure class='figure'>
<img class='figure-img img-fluid' src='images/SWITCHproj_3D01.jpg' style='width: 50%;'>
<img class='figure-img img-fluid' src='images/SWITCHproj_2D01.png' style='width: 50%;'>
</figure>
<p>The project will explore the problem of assisting a user to stand up and sit down. This problem will be modeled as two kinematic chains, where only planar movements will be considered. One kinematic chain will represent a humanoid robot and the other will represent the user to assist. It is assumed that both have fixed feet on the ground and are connected to a common endeffector point representing their hands, meaning that the resulting system is a closed kinematic chain in which only a few of the articulations can be controlled.</p> 
<p>The assistance skill consists of moving the user from a static sitting pose to a static standing pose through the contact point. It involves challenging aspects of anticipation, shared control, initiation of movements, leader-follower behaviors and haptic communication. This project will concentrate on studying the dynamical motion aspects to achieve such assistance, which requires the consideration of inertia and center of mass movements with respect to the feet.</p>
</div>

</div>




<hr><br>
<h4>SMARTPHONE INTERFACE USING AUGMENTED REALITY</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<figure class='figure'>
<img class='figure-img img-fluid' src='images/projProp-smartphone-interface01.png'>
</figure>
<p>We have developed an augmented reality interface running on Android smartphones to display a virtual robot (left image). The app relies on ARCore, Google's toolkit for building augmented reality applications on Android and iOS devices. This toolkit is used to estimate the location of the phone and to render 3D graphics on top of the camera's image displayed on the screen.</p>   
<p>The aim of the project is to extend this interface to let the user move the virtual robot to a desired configuration (see right images as an illustration). Several options can be considered, such as clicking on the robot articulations and dragging them to their desired positions, or drawing a stick figure on top of the image, which is then interpreted to set the desired pose of the robot.</p>
<p>The Android smartphone will be programmed in Java (knowledge of either Java or C++ is required for the project).</p>
<p>The developed interface will be tested to change the pose of a real 7-axis Panda robot (Franka Emika), by first setting and visualizing the motion of the virtual robot on the smartphone, and then running the motion on the real robot. A basic interface between the mobile phone and the robot is already available for the project (by using the ROS middleware). The proposed approach will finally be evaluated with inexperienced user to determine if it is accurate and easy-to-use.</p>
<p><b>Keywords:</b> augmented reality, smartphone interfaces, robotics, inverse kinematics, machine learning</p>
</div>

</div>



<hr><br>
<h4>TENSOR-VARIATE REGRESSION IN ROBOTICS APPLICATIONS</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<figure class='figure'>
<img class='figure-img img-fluid' src='images/tensor-factorization01.jpg'>
<figcaption class='figure-caption'>Left: Standard matrix factorization (singular value decomposition). Right: Tensor factorization.</figcaption>
</figure>
<p>Many regression problems in robotics can be formulated as that of collecting data in the form of input x and output y, which are then used to estimate a new output y', given a new input x'. This project proposes to extend regression to the more general case in which x and y are multidimensional arrays (also called tensors).</p> 
<p>Tensors are generalization of matrices to arrays of higher dimensions, where vectors and matrices correspond to 1st and 2nd-order tensors. When data are organized in matrices or arrays of higher dimensions (tensors), classical regression methods first transform these data into vectors, therefore ignoring the underlying structure of the data and increasing the dimensionality of the problem. This flattening operation typically leads to overfitting when only few training data are available.</p> 
<p>Tensor methods are well established in several machine learning fields such as image/video processing and recommendation systems, but this area of research is still at its infancy in robotics.</p> 
<p>The project proposes to test and evaluate different generalizations of the pseudoinverse for tensor-variate data (see references below), which will be exploited in the context of linear regression for robotics applications.</p>
<p><b>Keywords:</b> tensor methods, regression</p>
<p><b>References:</b></p>
<p><a href='https://arxiv.org/pdf/1701.01037.pdf' target='_blank'>Lock, E.F. (2018). Tensor-on-Tensor Regression. Computational and Graphical Statistics 27:3, pp. 638-647</a>.</p>
<p><a href='https://ieeexplore.ieee.org/document/8421595' target='_blank'>Yin, M., Gao, J., Xie, S. and Guo, Y. (2019). Multiview Subspace Clustering via Tensorial t-Product Representation. IEEE Trans. on Neural Networks and Learning Systems 30:3, pp. 851-864</a>.</p>
</div>

</div>






<hr><br>
<h4>TENSOR-VARIATE DICTIONARY LEARNING OF MOVEMENT PRIMITIVES</h4>

<br>
<div class='row'>

<div class='col-md-7'>
<p>In robotics, it is useful to represent movements as a superposition of basis functions. Often, the set of basis functions is predefined. The corresponding learning problem consists of estimating how to combine the set of basis functions to form a desired movement. The set of basis functions (dictionary of movement primitives) can be learned together with their combination. To do this, a sparse coding approach for dictionary learning will be exploited, as presented in the video and paper below. The approach will be evaluated on robot motion data.</p>
<p>The standard dictionary learning problem is formulated with data in the form of input x and output y, which are then used to estimate a new output y_new, given a new input x_new. The project proposes to extend this approach to the more general case in which x and y are multidimensional arrays (also called tensors, see slides below). The approach will be evaluated on robot motion data.</p>
<p><b>Keywords:</b> tensor methods, movement primitives, sparse coding</p>
</div>

<div class='col-md-5'>
<figure class='figure p-3'>
<img class='figure-img img-fluid' src='images/projProp-basisFunctions01.png'>
</figure>
<p><b>References:</b></p>

<p><a href='http://calinon.ch/misc/EE613/EE613-linearRegressionII.pdf' target='_blank'>Slides for course on tensor-variate regression (the slides contain also other topics)</a></p>
<p><a href='https://www.youtube.com/watch?v=7a0_iEruGoM&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&index=61&t=0s' target='_blank'>Sparse coding video lectures from Hugo Larochelle (part 8.1-8.9)</a></p>
<p><a href='https://www.di.ens.fr/willow/pdfs/icml09.pdf' target='_blank'>Julien Mairal, Francis Bach, Jean Ponce and Guillermo Sapiro, "Online Dictionary Learning for Sparse Coding", ICML'2019</a></p>
</div>

</div>


<hr><br>
<h4>SUBSPACE LEARNING FOR ROBOT CONTROL APPLICATIONS</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<p>In robotics, movements can be represented as a dynamical system describing the evolution in time of the robot. Most often, the system is nonlinear, and the standard approach is to linearize the system so that locally, a linear system can be considered. Another approach, originally proposed by Koopman, is to augment the original set of variables composing the state space so that the nonlinear system can be expressed as a linear system in this augmented state space.</p>
<p>To do this, several approaches have been proposed, such as forming this augmented state space with polynomial or Fourier expansions of the original signal, or learning this augmented state space with autoencoders. Often, these approaches consider systems that estimate the next state based on the current state.</p>
<p>A promising approach recently proposed in the reference below is to consider a history of the previous states. This approach suggests to go beyond standard polynomial basis functions or Fourier basis functions by exploiting delay coordinates as basis functions, in the form of a factorization of a Hankel matrix. The resulting algorithm is surprisingly short and simple to implement. This project proposes to explore this approach in a robot control task with the 7-axis Panda robot (Franka Emika).</p>
<p><b>Keywords:</b> dynamical systems, robot control, subspace learning, delay embedding, time series analysis</p>
<p><b>References:</b></p>
<p><a href='https://arxiv.org/pdf/1608.05306.pdf' target='_blank'>Steven L. Brunton, Bingni W. Brunton, Joshua L. Proctor, Eurika Kaiser, and J. Nathan Kutz, "Chaos as an Intermittently Forced Linear System", Nature Communications, 2017</a></p>
<p><a href='https://www.youtube.com/watch?v=831Ell3QNck' target='_blank'>Hankel Alternative View of Koopman (HAVOK) Analysis (video)</a></p>
</div>

</div>



<hr><br>
<h4>LEARNING TO SEARCH: AN ERGODIC CONTROL APPROACH</h4>

<br>
<div class='row'>

<div class='col-md-7'>
<p>In ergodic control, the aim is to find a sequence of control commands u(t) so that the retrieved trajectory x(t) covers a bounded space X in proportion of a given spatial distribution phi(x). The objective is formulated as a tracking problem in the frequency domain, in which the goal is to match the frequency patterns by putting more importance on low frequency components than high frequency components.</p>
<p>The resulting controller produces a natural movement to explore the regions of interest. Interestingly, this search pattern does not rely on random moves, but is instead well defined mathematically as a tracking problem in the frequency domain (i.e., matching Fourier series coefficients).</p>
<p>The aim of this project is to combine learning from demonstration with ergodic control, by extracting from the demonstration the regions that we need to explore, and then letting ergodic control explores these regions. The approach will be tested in an insertion task with a 7-axis Panda robot (Franka Emika). The implementation will be in C++ or Matlab (example codes for ergodic control in C++ and Matlab will be provided).</p>
<p><b>Keywords:</b> robot control, learning from demonstration, Fourier series</p>
<p><b>Reference:</b></p>
<p><a href='http://calinon.ch/papers/Calinon_MMchapter2019.pdf' target='_blank'>Calinon, S., "Mixture Models for the Analysis, Edition, and Synthesis of Continuous Time Series", Bouguila, N. and Fan, W. (eds), Mixture Models and Applications, Springer, 2019</a></p>
</div>

<div class='col-md-5'>
<figure class='figure p-3'>
<img class='figure-img img-fluid' src='images/projProp-ergodicControl01.png'>
</figure>
</div>

</div>





<hr><br>
<h4>CIGAR BOX JUGGLING WITH ROBOTS</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<figure class='figure p-3'>
<img class='figure-img img-fluid' src='images/projProp-cigarBoxes01.png'>
</figure>
<p>"Cigar box juggling" consists of manipulating three boxes, by holding two boxes while one box is free. It involves the juggler to skillfully move two of the boxes to change their configurations (see above sequence). For beginners, the movement is typically executed by moving the boxes up-and-down when changing the configuration, so that there is more time to change the orientation of the box being held (since the other hand will ``accompany'' the falling box). If the up-and-down motion has high amplitude, the movement to change the configuration is relatively slow.</p>
<p>This project proposes to explore whether such motion could be planned and executed on two 7-axis Panda robots (Franka Emika). The dynamical model will first be built to determine the motion to execute. Model predictive control will then be employed to control the robot with anticipation capability.</p>
<p><b>Keywords:</b> robot control, robot planning, skills learning and adaptation, model predictive control</p>
</div>

</div>


<!--
<hr><br>
<h4>GROWBOTHUB ROBOTICS PROJECTS</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<figure class='figure'>
<img class='figure-img img-fluid' src='images/growbothub01.jpg'>
</figure>
<p>The <a href='https://growbothub.space/' target='_blank'>GrowbotHub project</a> was founded by a team of EPFL/UNIL students from the Légumes Perchés Association and took part in the first edition of Igluna, an international student project to build a habitat demonstrator for sustaining life in an extreme environment such as on the Moon. Igluna is coordinated by the Swiss Space Center at EPFL and GrowbotHub is one of its most successful projects.</p>
<p>Three robotics projects using a <a href='https://www.kuka.com/en-ch/products/robotics-systems/industrial-robots/lbr-iiwa' target='_blank'>7-axis KUKA LBR iiwa</a> are described below. The full list of GrowbotHub projects is available <a href='http://growbothub.space/mesmerize/semester-projects/' target='_blank'>here</a>.</p>

<strong>Project 1.7 - Control of the Kuka robot in coordination with the rest of the system</strong>

<p>The goal of this project is to create a controller for the Kuka robot by replying on the sunrise environment provided by Kuka. Since the controller will need to communicate with the rest of the system, it is proposed to use <a href='https://www.ros.org/' target='_blank'>ROS</a> as a middleware to allow communication between the different parts of the system. In a first stage, the controller will rely on inverse kinematics to control the position and orientation of the tool. In a second stage, the torque control capability of the robot will be exploited to combine force tracking and position tracking.</p>

<strong>Project 1.8 - Adaptive pick-and-place of vegetable pots with the Kuka robot</strong>

<p>The goal of this project is to determine and evaluate solutions to pick-up vegetable pots (incl. how to modify the pots for better prehension), and drop them at a desired location. It requires to take into account motion planning, together with the limitations of the robot, including gripper and joint limits. Several gripper solutions and configurations will be investigated and tested.</p>

<strong>Project 1.9 - Perception and control for a 3-finger gripper for harvesting vegetables</strong>

<p>This project aims at harvesting vegetables (e.g., carrots) using a 3-finger gripper, by detecting the gripping point and applying appropriate forces to collect the vegetables without damaging them. There are two variants in this project, focusing on either the perception aspect or the control aspect. The first variant of the project will focus on determining a gripping point (position and orientation of the gripper) from computer vision processing. The second variant of the project will determine which gripper, gripper configuration and fingertips are required for the task, and will control the robot to reach the desired gripping point.</p>
</div>
</div>
-->


<!--
<hr><br>
<h4>HIDDEN MARKOV MODEL FOR MUSIC SCORE SYNCHRONIZATION IN A HUMAN-ROBOT COLLABORATION</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<figure class='figure p-3'>
<img class='figure-img img-fluid' src='images/projProp-robotPianist01.png'>
</figure>
<p>We are developing a human-robot collaboration experiment in which a person plays a melody on a piano while a robot plays the accompanying chords (see the robot setup in the above figure). For the synchronization to be effective, the robot has to anticipate its movements in accordance to the estimated tempo. </p>
<p>This project proposes to rely on the score (midi format) to initialize a hidden Markov model (HMM), which is then refined by the recordings of a person playing the same score. The goal is to encode the natural variations to improve the encoding and synthesis of the music track. This HMM will then be used in the experiment to estimate the time when the accompanying notes in the partition should be played by the robot. The approach will be implemented in Python or C++, by exploiting existing example codes in Matlab or C++ as starting point.</p>
<p><b>Keywords:</b> time series analysis, hidden Markov model (HMM), music synthesis, human-robot collaboration</p>
</div>

</div>
-->


<!--
<hr><br>
<h4>CONVOLUTIONAL NEURAL NETWORKS AND TENSOR METHODS</h4>

<br>
<div class='row'>

<div class='col-md-12'>
<figure class='figure p-3'>
<img class='figure-img img-fluid' src='images/projProp-TME01.png'>
</figure>
<p>Sensory data in robotics are typically organized as multidimensional arrays (arrays of sensors, multiple channels, time evolution of data, multiple coordinate systems, etc.). This led our group to investigate tensor methods (a field of research also referred to as multilinear algebra).</p>
<p>Tensors are generalization of matrices to arrays of higher dimensions, where vectors and matrices correspond to 1st and 2nd-order tensors. When data are organized in matrices or arrays of higher dimensions (tensors), classical regression methods first transform these data into vectors, therefore ignoring the underlying structure of the data and increasing the dimensionality of the problem. This flattening operation typically leads to overfitting when only few training data are available.</p>
<p>In the reference below, we investigated the use of a mixture of experts relying on tensorial representations. The aim is to take into account the underlying structure of the data, with an approach that remains efficient even when only few training data are available.</p>
<p>The left figure illustrates how ridge regression and logistic regression can be extended to tensor-variate data. The right figure shows that a tensor-variate mixture of experts can be built by using tensor regression as experts, and tensor logistic regression as gating functions.</p>
<p>The goal of this project is to compare this strategy against the use of a convolutional neural network (CNN), from both theoretical and practical perspectives. For this project, previous experience on deep learning (and in particular CNN) would be useful. The implementation can be either done with TensorFlow, PyTorch or Matlab.</p>
<p><b>Keywords:</b> convolutional neural network, tensor methods</p>
<p><b>Reference:</b></p>
<p><a href='http://calinon.ch/papers/Jaquier-arXiv2019.pdf' target='_blank'>Jaquier, N., Haschke, R. and Calinon, S., "Tensor-variate Mixture of Experts", arXiv:1902.11104, 2019</a></p>
</div>

</div>
-->


<!--
<hr><br>
<h4>ONLINE IMITATION WITH OBJECT-AWARE MAPPING</h4>

<br>
<div class='row'>

<div class='col-md-7'>
<p>Online imitation consists of copying a motion in real-time (see above image). This problem is very challenging in robotics because the robot and the user have different kinematics (and dynamics) characteristics, requiring the mapping strategy to be carefully chosen. Imitating joint angle trajectories is usually not appropriate, because the joints of the robot are not necessary organized in the same way as the morphology of a person. For this reason, the most often considered mapping is to copy the position of the robot/user hands, described in the Cartesian space.</p>
<p>In this project, it is proposed to explore a new strategy that consists of creating a mapping considering the objects in the robot/user workspace. This idea arises from the observation that most of the time, the robot hands will be used to manipulate objects, and the locations of these objects will typically differ in the two workspaces (the one of the user, and the one of the robot). In order to cope with this mapping problem, it is proposed to represent the movement from the perspective of these objects (e.g., in a relative distance space to the objects). Several strategies will be investigated and tested with a 7-axis Panda robot (Franka Emika). The implementation will be done either in Python, C++ or Matlab.</p>
<p><b>Keywords:</b> imitation, learning from demonstration, motion retargeting, correspondence problems</p>
<p><b>Reference:</b></p>
<p><a href='http://calinon.ch/papers/Calinon-EncyclopediaRobotics2019.pdf' target='_blank'>Calinon, S., ``Learning from Demonstration (Programming by Demonstration)'', Ang, M.H., Khatib, O. and Siciliano, B. (eds), Encyclopedia of Robotics, Springer, 2019</a></p>
</div>

<div class='col-md-5'>
<figure class='figure p-3'>
<img class='figure-img img-fluid' src='images/projProp-BaxterImitation01.png'>
</figure>
</div>

</div>
-->

<!--

---
Optimal control with keypoints learned from demonstration

Kinesthetic teaching consists of grabbing the robot arm to demonstrate a task while the robot collects proprioceptive information through its joint encoders. Demonstrating a full movement with kinesthetic teaching is cumbersome and often loses the fluidity and dynamics in the movement. This project proposes to use kinesthetic teaching only for defining a set of viapoints that the robot should pass through, and to rely on optimization to let the robot compute control commands to pass through the desired viapoints. To do this, a spacetime formulation of iLQR will be tested, which consists of iteratively solving linear quadratic regulation problems, resulting in a planning strategy that takes into account both time and space aspects to fulfill the task. 

The project will use a 7-axis Panda robot form the Franka Emika company. The approach will be tested in a set of manipulation tasks inspired by industrial applications.


---
Adaptive robot control to pour liquids from a bottle 

In manipulation, pouring skills are challenging because the generated movement and feedback depend on the quantity of liquid present in the container. We plan to study this skill in the context of a robot application in which a manipulator, holding a bottle filled with liquid, can pour the liquid into a glass. Such challenge requires the combination of several sources of perception, including proprioceptive information (estimating the remaining quantity of liquid in the bottle from its weight), and vision (as a way to correct the pouring behavior, as well as to determine when to stop pouring). 

The project proposes to study this challenge by using a 7-axis Panda robot from the Franka Emika company (https://franka.de/). This robot is endowed with torque sensors, which will be exploited to estimate the quantity of liquid in the bottle form its weight. This estimate will be used in a planning step to define an open-loop movement (anticipated movement). This movement will then be corrected during its execution, with the help of a simple vision system that will be developed in the project to determine when colored liquid pours out of the bottle (the location of the top of the bottle is known by proprioception, using the joint angle encoders of the robot). The controller will be developed in C++, by using the libfranka library (https://frankaemika.github.io/docs/overview.html).

---
Obstacle-aware inverse kinematics with implicit surface representation

Gaussian Process Implicit Surfaces (GPIS) is a technique allowing shapes to be modeled in a probabilistic manner. It relies on a set of datapoints annotated with values 0, 1 and -1 to define if the datapoint is on the border, inside or outside the shape. We can then use this model to estimate the distance of a point to the border of the shape, as well as estimating the direction to move toward or away from this border. This direction can be weighted with respect to the distance to the border, in order to obtain a vector field that is strong when close to the border (or inside the shape), and weak elsewhere. The distance information takes the form of a Gaussian distribution, meaning that we also get uncertainty information about this estimate. Thus, such estimates will be more trustworthy in areas covered by labeled points than in areas that were not demonstrated. The probabilistic formulation can also be exploited to define prior information about the shape, which is useful when only a few points is available to model the shape, or when datapoints representing a portion of the shape are missing. 

The GPIS approach is typically used in computer graphics to represent 3D shapes or 2D contour, but the approach can be used in any dimension, including the configuration space (joint angle space) of a robot. In this project, it is proposed to rely on this representation to let a robot move from any two points while avoiding fixed obstacles in the robot workspace, where GPIS is used to model the contours of the obstacles. To do this, the robot is first moved in the free regions of its workspace to collect datapoints with labels -1. It is then moved on the boundary of the admissible regions (labels 0). The points of the forbidden regions are then either demonstrated (when possible), or computed as centroids of the contour points (or groups of contour points). Two variants are then proposed for the controller:

GPIS in configuration space (variant 1):
In this variant, GPIS is modeled in a 7D space (corresponding to the 7 axes of the robot manipulator). A standard inverse kinematics (IK) solver is used at each time step to compute a velocity command in joint angle space. This command is then combined with the repulsive command provided by GPIS, either computed as a product of Gaussians (fusion of the two commands), or as a hierarchical control problem (by exploiting the nullspace matrix as an operator to prioritize obstacle avoidance).

GPIS in operational space (variant 2):
In this variant, GPIS is modeled in a 3D space (Cartesian coordinate system describing the task space of the robot). Instead of using a standard IK solver that takes into account only the end-effector of the robot, we extend the forward kinematics function to take into account several points on the kinematic chain of the robot (for example, each articulation). The gradient of this function is a Jacobian matrix, whose pseudoinverse defines a mapping between the desired velocities of these points in task space and the velocity commands in joint space. The desired velocities are determined by GPIS (collision avoidance for all points) and by the task to achieve (endeffector tracking a target). This inverse kinematics formulation can be extended to a weighted least-squares approach in which the weights are set with respect to the estimated distance of the points to the contour, so that the points on the kinematic chain that are far from obstacles will not have an impact on the resulting joint angle commands.

Both variants will first be tested with a 2-axis planar robot using the Matlab robotics toolbox (https://github.com/petercorke/robotics-toolbox-matlab or http://www.petercorke.com/RTB/). For inverse kinematics, the project will rely on demo_IK01.m, a simple IK example (incl. nullspace control), available in https://gitlab.idiap.ch/rli/pbdlib-matlab/. For GPIS, the project will rely on demo_GPR_closedShape02.m, also available in https://gitlab.idiap.ch/rli/pbdlib-matlab/.

References:
GPIS paper: https://www.microsoft.com/en-us/research/publication/gaussian-process-implicit-surfaces-2/ 
GPR: slides 3-27 from http://calinon.ch/misc/EE613/EE613-nonlinearRegressionII.pdf 
IK: slides 22-27 from http://calinon.ch/misc/EE613/EE613-linearRegressionI.pdf 
-->


</div><!-- /.main-template -->
</div><!-- /.container -->


<!-- Bootstrap core JavaScript placed at the end of the document so the pages load faster -->
<script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>
<script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
<script src="js/bootstrap.min.js"></script>
<!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
<!--<script src="assets/js/ie10-viewport-bug-workaround.js"></script>-->
</body>
</html>
