<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1, shrink-to-fit=no'>
<meta name='description' content='Sylvain Calinon's personal website'>
<meta name='author' content='Sylvain Calinon'>
<meta name='keywords' content='Sylvain Calinon, robotics, machine learning, robot learning, human-robot interaction, human-robot collaboration, learning from demonstration, programming by demonstration, artificial intelligence'>
<meta name='theme-color' content='#292b2c'>
<link rel='icon' href='images/favicon.ico'>
<title>Sylvain Calinon</title>
<link href='css/bootstrap.min.css' rel='stylesheet'>
<link href='css/main-template.css' rel='stylesheet'>
<link href='font-awesome/css/font-awesome.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css?family=Lobster|Raleway' rel='stylesheet'> 
<script type='text/x-mathjax-config'>
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script type='text/javascript' async src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
</head>
<body>
<nav class='navbar navbar-expand-lg navbar-dark bg-dark fixed-top'>
<button class='navbar-toggler navbar-toggler-right' type='button' data-toggle='collapse' data-target='#navbarsExampleDefault' aria-controls='navbarsExampleDefault' aria-expanded='false' aria-label='Toggle navigation'>
<span class='navbar-toggler-icon'></span>
</button>
<a class='navbar-brand' href='index.htm'>
<h3 style='display: inline-block;'>Sylvain Calinon</h3>
<img src='images/pbd-thumbnail02.png' style='display: inline-block; margin-left: 20px;'>
</a>

<div class='collapse navbar-collapse' id='navbarsExampleDefault'>
<ul class='navbar-nav mr-auto'>
<li class='nav-item'><a class='nav-link' href='index.htm'>News</a></li>
<li class='nav-item'><a class='nav-link' href='cv.htm'>CV</a></li>
<li class='nav-item'><a class='nav-link' href='research.htm'>Research</a></li>
<!--li class='nav-item active'><a class='nav-link' href='publications.htm'>Publications <span class='sr-only'>(current)</span></a></li>-->
<li class='nav-item dropdown'><a class='nav-link dropdown-toggle' href='#' id='dropdown_pub' data-toggle='dropdown' aria-haspopup='true' aria-expanded='false'>Publications <span class='sr-only'>(current)</span></a>
<div class='dropdown-menu' aria-labelledby='dropdown_pub'>
<a class='dropdown-item' href='publications.htm?#2017'>2017</a>
<a class='dropdown-item' href='publications.htm?#2016'>2016</a>
<a class='dropdown-item' href='publications.htm?#2015'>2015</a>
<a class='dropdown-item' href='publications.htm?#2014'>2014</a>
<a class='dropdown-item' href='publications.htm?#2013'>2013</a>
<a class='dropdown-item' href='publications.htm?#2012'>2012</a>
<a class='dropdown-item' href='publications.htm?#2011'>2011</a>
<a class='dropdown-item' href='publications.htm?#2010'>2010 and older</a>
</div>
</li>
<li class='nav-item'><a class='nav-link' href='teaching.htm'>Teaching</a></li>
<li class='nav-item'><a class='nav-link' href='book.htm'>Book</a></li>
<li class='nav-item'><a class='nav-link' href='videos.htm'>Videos</a></li>
<li class='nav-item'><a class='nav-link' href='codes.htm'>Codes</a></li>
<li class='nav-item'><a class='nav-link' href='open-positions.htm'>Open positions </a></li>
<li class='nav-item'><a class='nav-link' href='contact.htm'>Contact/Links</a></li>	
</ul>
</div>
</nav>

<div class='container'>
<div class='main-template'>

<div class='row'>
<div class='col-md-2 text-center'><img src='images/3034.jpg'></div>
<div class='col-md-10'>
<h5>Rozo, L., Calinon, S., Caldwell, D.G., Jimenez, P. and Torras, C. (2013)</h5>
<h5><strong>Learning collaborative impedance-based robot behaviors</strong></h5> 
<h5>In Proc. of the AAAI Conference on Artificial Intelligence, Bellevue, WA, USA, pp. 1422-1428.</h5>
<div class='row'>
<div class='col-md-4'>
<a href='papers/Rozo-AAAI2013.pdf' target='_BLANK'><i class='fa fa-file-pdf-o fa-fw'></i> Download as PDF</a>
</div>
<div class='col-md-4'>
<a href='http://www.aaai.org/Conferences/AAAI/aaai13.php' target='_BLANK'><i class='fa fa-external-link fa-fw'></i> Go to the publisher's website</a>
</div>
<div class='col-md-4'>
<a href='papers/Rozo-AAAI2013.bib' class='bibtex' target='_BLANK'><i class='fa fa-file-text-o fa-fw'></i> Save the reference as bib file</a>
</div>

</div></div></div>

<h3>Abstract</h3>
<p>Research in learning from demonstration has focused
on transferring movements from humans to robots.
However, a need is arising for robots that do not just
replicate the task on their own, but that also interact
with humans in a safe and natural way to accomplish
tasks cooperatively. Robots with variable impedance capabilities opens the door to new challenging applications, where the learning algorithms must be extended
by encapsulating force and vision information. In this
paper we propose a framework to transfer impedance-based behaviors to a torque-controlled robot by kinesthetic teaching. The proposed model encodes the exam-
ples as a task-parameterized statistical dynamical system, where the robot impedance is shaped by estimating
virtual stiffness matrices from the set of demonstrations.
A collaborative assembly task is used as testbed. The
results show that the model can be used to modify the
robot impedance along task execution to facilitate the
collaboration, by triggering stiff and compliant behaviors in an on-line manner to adapt to the user's actions.
</p>
	
<h3>Bibtex reference</h3>
<pre>@inproceedings{Rozo13AAAI,
  author="Rozo, L. and Calinon, S. and Caldwell, D. G. and Jimenez, P. and Torras, C.",
  title="Learning collaborative impedance-based robot behaviors",
  booktitle="{AAAI} Conference on Artificial Intelligence",
  year="2013",
  address="Bellevue, WA, USA",
  pages="1422--1428"
}
</pre>
	
<h3>Video</h3>
<p>This video shows the result of a learning by imitation approach that allows two users to demonstrate an assembly skill requiring different levels of compliance. Each furniture item to assemble will have specific characteristic that needs that are transferred to the robot. Re-programming the robot for each new item would not be possible. Here, the robot can learn this skill by demonstration. One user is grasping the robot and moving it by hand to demonstrate how it should collaborate with another user (kinesthetic teaching). A force sensor mounted at the wrist of the robot and a marker-based vision tracking system is used to record the position and orientation of the table legs that need to be mounted at four different point on the table top. After demonstration, the robot learns that it should first be compliant to let the user re-orient the table top in a comfortable pose to screw the corresponding table leg. Once the user starts to screw the leg, the robot becomes stiff to facilitate the task. This behavior is not pre-programmed, but is instead learn by the robot by extracting the regularities of the task from multiple demonstrations.</p>

<p>Credits: Leonel Rozo, Sylvain Calinon</p>

<center>
<iframe src="https://player.vimeo.com/video/63639925?title=0&byline=0&portrait=0" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</center>
<br>

	
<h3>Source codes</h3>
<p><p>Training of a task-parameterized Gaussian mixture model (GMM) based on candidate frames of reference.
The proposed task-parameterized GMM approach relies on the linear transformation and product properties of
Gaussian distributions to derive an expectation-maximization (EM) algorithm to train the model.
The proposed approach is contrasted with an implementation of the approach proposed by Wilson and Bobick in
1999, with an implementation applied to GMM (that we will call PGMM) and following the model described in
"Parametric Hidden Markov Models for Gesture Recognition", IEEE Trans. on Pattern Analysis and Machine
Intelligence.<br>
In contrast to the standard PGMM approach, the new approach that we propose allows the parameterization of
both the centers and covariance matrices of the Gaussians. It has been designed for targeting problems in
which the task parameters can be represented in the form of coordinate systems, which is for example the
case in robot manipulation problems.</p>  

<h4>Download</h4>
<p><img class='miniicon' src='minipng/compress.png'>
<a href="download/task-parameterized-GMM.zip">Download task-parameterized GMM <b>Matlab</b> sourcecode</a></p>
<p><img class='miniicon' src='minipng/compress.png'>
<a href="download/task-parameterized-GMM-cpp-cmdLine.zip">Download task-parameterized GMM <b>C++ (command line version)</b> sourcecode</a></p>
<p>(see also <a href='sourcecodes.php#DMP learned by GMR'>DMP LEARNED BY GMR</a> sourcecode)</p>

<h4>Usage</h4>
<p>For the Matlab version, unzip the file and run 'demo1' or 'demo2' in Matlab.<br>
For the C++ version, unzip the file and follow the instructions in the ReadMe.txt file.</p>

<h4>Reference</h4>
<ul><li>Calinon, S., Li, Z., Alizadeh, T., Tsagarakis, N.G. and Caldwell, D.G. (2012) <b>Statistical dynamical systems for skills acquisition in humanoids</b>. Proc. of the IEEE Intl Conf. on Humanoid Robots (Humanoids).</li></ul>
<br>

<h4>Demo 1 - Simple example of task-parameterized GMM learning and comparison with standard PGMM</h4>

<a href="images/task-parameterized-GMM.png" rel="lightbox">
<img class="widethumb" src='images/task-parameterized-GMM-thumbnail.png'>
</a>

<p>This example uses 3 trajectories demonstrated in a frame of reference that varies from one demonstration
to the other. A model of 3 Gaussian components is used to encode the data in the different frames, by providing the
parameters of the coordinate systems as inputs (transformation matrix A and offset vector b).</p>

<br>

<h4>Demo 2 - Example of task-parameterized movement learning with DS-GMR (statistical dynamical systems based on Gaussian mixture regression) </h4>

<a href="images/task-parameterized-GMM2.png" rel="lightbox">
<img class="widethumb" src='images/task-parameterized-GMM2-thumbnail.png'>
</a>

<p>This demo shows how the approach can be combined with the DS-GMR model to learn movements modulated
with respect to different frames of reference. The DS-GMR model is a statistical dynamical system approach
to learn and reproduce movements with a superposition of virtual spring-damper systems
retrieved by Gaussian mixture regression (GMR). For more details, see the 'DMP-learned-by-GMR-v1.0' example
code downloadable from the website below.</p></p>
	<center><a class='btn btn-primary' href='publications.htm'>Go back to the list of publications</a></center></div><!-- /.main-template -->
</div><!-- /.container -->


<!-- Bootstrap core JavaScript placed at the end of the document so the pages load faster -->
<script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>
<script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
<script src="js/bootstrap.min.js"></script>
<!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
<!--<script src="assets/js/ie10-viewport-bug-workaround.js"></script>-->
</body>
</html>

	


